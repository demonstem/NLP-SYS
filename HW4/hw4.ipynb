{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxOTS6n1ikVL"
   },
   "source": [
    "# HW 4 - POS Tagging with Hugging Face\n",
    "\n",
    "In this exercise, you will create a part-of-speech (POS) tagging system for Thai text using NECTEC’s ORCHID corpus. Instead of building your own deep learning architecture from scratch, you will leverage a pretrained tokenizer and a pretrained token classification model from Hugging Face.\n",
    "\n",
    "We have provided some starter code for data cleaning and preprocessing in this notebook, but feel free to modify those parts to suit your needs. You are welcome to use additional libraries (e.g., scikit-learn) as long as you incorporate the pretrained Hugging Face model. Specifically, you will need to:\n",
    "\n",
    "1. Load a pretrained tokenizer and token classification model.\n",
    "2. Fine-tune it on the ORCHID corpus for POS tagging.\n",
    "3. Evaluate and report the performance of your model on the test data.\n",
    "\n",
    "### Don't forget to change hardware accelrator to GPU in runtime on Google Colab ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQ1Uqldlj81G"
   },
   "source": [
    "## 1. Setup and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:44:20.761557Z",
     "iopub.status.busy": "2025-02-02T13:44:20.761229Z",
     "iopub.status.idle": "2025-02-02T13:44:37.065637Z",
     "shell.execute_reply": "2025-02-02T13:44:37.064434Z",
     "shell.execute_reply.started": "2025-02-02T13:44:20.761527Z"
    },
    "id": "kyb4FhsEEeH8",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: peft==0.10.0 in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (4.30.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (1.2.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (0.27.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2024.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.10.0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.10.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.10.0) (0.13.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft==0.10.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft==0.10.0) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft==0.10.0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft==0.10.0) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2024.12.14)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft==0.10.0) (2024.2.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install transformers and thai2transformers\n",
    "!pip install wandb\n",
    "!pip install -q transformers==4.30.1 datasets evaluate thaixtransformers\n",
    "!pip install -q emoji pythainlp sefr_cut tinydb seqeval sentencepiece pydantic jsonlines\n",
    "!pip install peft==0.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTgw8WW3BxWZ"
   },
   "source": [
    "## Setup\n",
    "\n",
    "1. Register [Wandb account](https://wandb.ai/login?signup=true) (and confirm your email)\n",
    "\n",
    "2. `wandb login` and copy paste the API key when prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:45:45.834149Z",
     "iopub.status.busy": "2025-02-02T13:45:45.833819Z",
     "iopub.status.idle": "2025-02-02T13:45:45.928965Z",
     "shell.execute_reply": "2025-02-02T13:45:45.928072Z",
     "shell.execute_reply.started": "2025-02-02T13:45:45.834123Z"
    },
    "id": "4FjMIqhTBfOX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !wandb \n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "wandb_api_key = user_secrets.get_secret(\"wandb_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:45:48.815614Z",
     "iopub.status.busy": "2025-02-02T13:45:48.815235Z",
     "iopub.status.idle": "2025-02-02T13:45:55.819671Z",
     "shell.execute_reply": "2025-02-02T13:45:55.818813Z",
     "shell.execute_reply.started": "2025-02-02T13:45:48.815560Z"
    },
    "id": "4qSh7MXZB74z",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdemonstem\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-BR7danGv6W"
   },
   "source": [
    "We encourage you to login to your `Hugging Face` account so you can upload and share your model with the community. When prompted, enter your token to login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:45:57.601952Z",
     "iopub.status.busy": "2025-02-02T13:45:57.601439Z",
     "iopub.status.idle": "2025-02-02T13:45:57.845842Z",
     "shell.execute_reply": "2025-02-02T13:45:57.844908Z",
     "shell.execute_reply.started": "2025-02-02T13:45:57.601925Z"
    },
    "id": "n7h8NENllZK2",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d03daa0b2b432199d82d6ea4579c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqkDkseilv19"
   },
   "source": [
    "Download the dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:46:42.040195Z",
     "iopub.status.busy": "2025-02-02T13:46:42.039864Z",
     "iopub.status.idle": "2025-02-02T13:46:48.225840Z",
     "shell.execute_reply": "2025-02-02T13:46:48.224711Z",
     "shell.execute_reply.started": "2025-02-02T13:46:42.040166Z"
    },
    "id": "kRksERXFEngl",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for Thichow/orchid_corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Thichow/orchid_corpus.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed94043ca0e424aa75715671a0e2754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/5.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b59ff4f3bd64cdd9863f257ef126352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e67b2eb9584f1684f6a0ced1b030de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581c3c0df3b34245b9f09fde33dbdd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "orchid = load_dataset(\"Thichow/orchid_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:46:50.859469Z",
     "iopub.status.busy": "2025-02-02T13:46:50.859141Z",
     "iopub.status.idle": "2025-02-02T13:46:50.865124Z",
     "shell.execute_reply": "2025-02-02T13:46:50.864177Z",
     "shell.execute_reply.started": "2025-02-02T13:46:50.859445Z"
    },
    "id": "T_AWd4d5lCYd",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n",
       "        num_rows: 18500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n",
       "        num_rows: 4625\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orchid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:46:53.601447Z",
     "iopub.status.busy": "2025-02-02T13:46:53.601126Z",
     "iopub.status.idle": "2025-02-02T13:46:53.608946Z",
     "shell.execute_reply": "2025-02-02T13:46:53.608137Z",
     "shell.execute_reply.started": "2025-02-02T13:46:53.601423Z"
    },
    "id": "QNmIqSo0FkAx",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'label_tokens': ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1'],\n",
       " 'pos_tags': [21, 39, 26, 26, 37, 4, 18],\n",
       " 'sentence': 'การประชุมทางวิชาการ ครั้งที่ 1'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orchid['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:46:55.655468Z",
     "iopub.status.busy": "2025-02-02T13:46:55.655067Z",
     "iopub.status.idle": "2025-02-02T13:46:55.661283Z",
     "shell.execute_reply": "2025-02-02T13:46:55.660400Z",
     "shell.execute_reply.started": "2025-02-02T13:46:55.655423Z"
    },
    "id": "hUuz3dLGlI_S",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'การประชุมทางวิชาการ ครั้งที่ 1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orchid['train'][0][\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:46:57.717820Z",
     "iopub.status.busy": "2025-02-02T13:46:57.717441Z",
     "iopub.status.idle": "2025-02-02T13:46:57.723611Z",
     "shell.execute_reply": "2025-02-02T13:46:57.722656Z",
     "shell.execute_reply.started": "2025-02-02T13:46:57.717794Z"
    },
    "id": "bh7fX19zI85W",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'การประชุมทางวิชาการ ครั้งที่ 1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(orchid['train'][0]['label_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:46:59.515343Z",
     "iopub.status.busy": "2025-02-02T13:46:59.515022Z",
     "iopub.status.idle": "2025-02-02T13:46:59.521248Z",
     "shell.execute_reply": "2025-02-02T13:46:59.520392Z",
     "shell.execute_reply.started": "2025-02-02T13:46:59.515317Z"
    },
    "id": "38jM9YcSFmjV",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total type of pos_tags : 47\n",
      "['ADVI', 'ADVN', 'ADVP', 'ADVS', 'CFQC', 'CLTV', 'CMTR', 'CMTR@PUNC', 'CNIT', 'CVBL', 'DCNM', 'DDAC', 'DDAN', 'DDAQ', 'DDBQ', 'DIAC', 'DIAQ', 'DIBQ', 'DONM', 'EAFF', 'EITT', 'FIXN', 'FIXV', 'JCMP', 'JCRG', 'JSBR', 'NCMN', 'NCNM', 'NEG', 'NLBL', 'NONM', 'NPRP', 'NTTL', 'PDMN', 'PNTR', 'PPRS', 'PREL', 'PUNC', 'RPRE', 'VACT', 'VATT', 'VSTA', 'XVAE', 'XVAM', 'XVBB', 'XVBM', 'XVMM']\n"
     ]
    }
   ],
   "source": [
    "label_list = orchid[\"train\"].features[f\"pos_tags\"].feature.names\n",
    "print('total type of pos_tags :', len(label_list))\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:47:01.453712Z",
     "iopub.status.busy": "2025-02-02T13:47:01.453332Z",
     "iopub.status.idle": "2025-02-02T13:47:25.453100Z",
     "shell.execute_reply": "2025-02-02T13:47:25.452112Z",
     "shell.execute_reply.started": "2025-02-02T13:47:01.453682Z"
    },
    "id": "Uf_NDWg7F6z_",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e73c0f0ad904970a60f4a5d4a2b73fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "#transformers\n",
    "from transformers import (\n",
    "    CamembertTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "#thaixtransformers\n",
    "from thaixtransformers import Tokenizer\n",
    "from thaixtransformers.preprocess import process_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1suScqmntBW"
   },
   "source": [
    "Next, we load a pretrained tokenizer from Hugging Face. In this work, we utilize WangchanBERTa, a Thai-specific pretrained model, as the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jt3ASYUVm54n"
   },
   "source": [
    "# Choose Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFBKLqbIm23-"
   },
   "source": [
    "In this notebook, you can choose from 5 versions of WangchanBERTa, XLMR and mBERT to perform downstream tasks on Thai datasets. The datasets are:\n",
    "\n",
    "* `wangchanberta-base-att-spm-uncased` (recommended) - Largest WangchanBERTa trained on 78.5GB of Assorted Thai Texts with subword tokenizer SentencePiece\n",
    "* `xlm-roberta-base` - Facebook's [XLMR](https://arxiv.org/abs/1911.02116) trained on 100 languages\n",
    "* `bert-base-multilingual-cased` - Google's [mBERT](https://arxiv.org/abs/1911.03310) trained on 104 languages\n",
    "* `wangchanberta-base-wiki-newmm` - WangchanBERTa trained on Thai Wikipedia Dump with PyThaiNLP's word-level tokenizer  `newmm`\n",
    "* `wangchanberta-base-wiki-syllable` - WangchanBERTa trained on Thai Wikipedia Dump with PyThaiNLP's syllabel-level tokenizer `syllable`\n",
    "* `wangchanberta-base-wiki-sefr` - WangchanBERTa trained on Thai Wikipedia Dump with word-level tokenizer  `SEFR`\n",
    "* `wangchanberta-base-wiki-spm` - WangchanBERTa trained on Thai Wikipedia Dump with subword-level tokenizer SentencePiece\n",
    "\n",
    "In the first part, we require you to select the wangchanberta-base-att-spm-uncased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HbZo_TZDn17"
   },
   "source": [
    "<b> Learn more about using wangchanberta at [wangchanberta_getting_started_ai_reseach](https://colab.research.google.com/github/PyThaiNLP/thaixtransformers/blob/main/notebooks/wangchanberta_getting_started_aireseach.ipynb?fbclid=IwY2xjawH61XZleHRuA2FlbQIxMAABHZUaAmHobzmCMHpX0EgdLdjDAEwSX0bjqpo5xPUSIx9b4O_dsIvvG8KVNA_aem_IyKkvzy-VPf9k2pYAFf6Nw#scrollTo=n5IaCot9b3cF) <b>\n",
    "\n",
    "\n",
    "\n",
    "*   You need to set the transformers version to transformers==4.30.1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kl2NposVIh9-"
   },
   "source": [
    "`In the first part, we require you to select the wangchanberta-base-att-spm-uncased.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:47:46.316399Z",
     "iopub.status.busy": "2025-02-02T13:47:46.315562Z",
     "iopub.status.idle": "2025-02-02T13:47:47.049359Z",
     "shell.execute_reply": "2025-02-02T13:47:47.048478Z",
     "shell.execute_reply.started": "2025-02-02T13:47:46.316368Z"
    },
    "id": "n5IaCot9b3cF",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39453db29f2142bcb1ac9df252f2065d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/905k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e489a7be7a4072a0b851026eaa8076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/282 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1ea6ef825745fcbdc4155e9ec96040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
      "The class this function is called from is 'WangchanbertaTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
      "The class this function is called from is 'WangchanbertaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "model_names = [\n",
    "    'airesearch/wangchanberta-base-att-spm-uncased',\n",
    "    'airesearch/wangchanberta-base-wiki-newmm',\n",
    "    'airesearch/wangchanberta-base-wiki-ssg',\n",
    "    'airesearch/wangchanberta-base-wiki-sefr',\n",
    "    'airesearch/wangchanberta-base-wiki-spm',\n",
    "]\n",
    "\n",
    "#@title Choose Pretrained Model\n",
    "model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "\n",
    "#create tokenizer\n",
    "tokenizer = Tokenizer(model_name).from_pretrained(\n",
    "                f'{model_name}',\n",
    "                revision='main',\n",
    "                model_max_length=416,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzdbERHLwd0X"
   },
   "source": [
    "Let's try using a pretrained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:47:50.197588Z",
     "iopub.status.busy": "2025-02-02T13:47:50.197190Z",
     "iopub.status.idle": "2025-02-02T13:47:50.205467Z",
     "shell.execute_reply": "2025-02-02T13:47:50.204286Z",
     "shell.execute_reply.started": "2025-02-02T13:47:50.197542Z"
    },
    "id": "qwrwXsHFwl-G",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : ศิลปะไม่เป็นเจ้านายใคร และไม่เป็นขี้ข้าใคร\n",
      "tokens : ['<s>', '', 'ศิลปะ', 'ไม่เป็น', 'เจ้านาย', 'ใคร', '<_>', 'และ', 'ไม่เป็น', 'ขี้ข้า', 'ใคร', '</s>']\n"
     ]
    }
   ],
   "source": [
    "text = 'ศิลปะไม่เป็นเจ้านายใคร และไม่เป็นขี้ข้าใคร'\n",
    "print('text :', text)\n",
    "tokens = []\n",
    "for i in tokenizer([text], is_split_into_words=True)['input_ids']:\n",
    "  tokens.append(tokenizer.decode(i))\n",
    "print('tokens :', tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEQAVqO8pDhK"
   },
   "source": [
    "model : * `wangchanberta-base-att-spm-uncased`\n",
    "\n",
    "First, we print examples of label tokens from our dataset for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:47:52.257837Z",
     "iopub.status.busy": "2025-02-02T13:47:52.257317Z",
     "iopub.status.idle": "2025-02-02T13:47:52.265911Z",
     "shell.execute_reply": "2025-02-02T13:47:52.264934Z",
     "shell.execute_reply.started": "2025-02-02T13:47:52.257797Z"
    },
    "id": "Vw_GdRdlpAhu",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 0\n",
      "label_tokens : ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']\n",
      "pos_tags : [21, 39, 26, 26, 37, 4, 18]\n",
      "sentence : การประชุมทางวิชาการ ครั้งที่ 1\n"
     ]
    }
   ],
   "source": [
    "example = orchid[\"train\"][0]\n",
    "for i in example :\n",
    "    print(i, ':', example[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTuiwEWkppdA"
   },
   "source": [
    "Then, we use the sentence 'การประชุมทางวิชาการ<space>ครั้งที่ 1' to be tokenized by the pretrained tokenizer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:47:54.059554Z",
     "iopub.status.busy": "2025-02-02T13:47:54.059185Z",
     "iopub.status.idle": "2025-02-02T13:47:54.065559Z",
     "shell.execute_reply": "2025-02-02T13:47:54.064806Z",
     "shell.execute_reply.started": "2025-02-02T13:47:54.059524Z"
    },
    "id": "BRCxMtHToN16",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [5, 10, 882, 8222, 8, 10, 1014, 8, 10, 59, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'การประชุมทางวิชาการ ครั้งที่ 1'\n",
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxi8WqZnGa5F"
   },
   "source": [
    "These are already mapped into discrete values. We can uncover the original token text from the tokens by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:47:56.177465Z",
     "iopub.status.busy": "2025-02-02T13:47:56.177092Z",
     "iopub.status.idle": "2025-02-02T13:47:56.185733Z",
     "shell.execute_reply": "2025-02-02T13:47:56.184505Z",
     "shell.execute_reply.started": "2025-02-02T13:47:56.177436Z"
    },
    "id": "optGK_eco3K6",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "▁\n",
      "การประชุม\n",
      "ทางวิชาการ\n",
      "<_>\n",
      "▁\n",
      "ครั้งที่\n",
      "<_>\n",
      "▁\n",
      "1\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "for i in tokenizer(text)['input_ids']:\n",
    "  print(tokenizer.convert_ids_to_tokens(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3l13UKnwK-d"
   },
   "source": [
    "Now let's look at another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:47:58.671437Z",
     "iopub.status.busy": "2025-02-02T13:47:58.671116Z",
     "iopub.status.idle": "2025-02-02T13:47:58.679696Z",
     "shell.execute_reply": "2025-02-02T13:47:58.678831Z",
     "shell.execute_reply.started": "2025-02-02T13:47:58.671413Z"
    },
    "id": "UyfIR3BowU84",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (Bilingual transfer dictionary)\n",
      "tokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', '<unk>', 'i', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\n",
      "label tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'Bilingual transfer dictionary', ')']\n",
      "label pos : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n"
     ]
    }
   ],
   "source": [
    "example = orchid[\"train\"][1899]\n",
    "print('sentence :', example[\"sentence\"])\n",
    "tokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print('tokens :',tokens)\n",
    "print('label tokens :', example[\"label_tokens\"])\n",
    "print('label pos :', example[\"pos_tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmV6M-vAwew5"
   },
   "source": [
    "Notice how `B` becomes an ``<unk>`` token. This is because this is an uncased model, meaning it only handles small English characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WniJR47ww7a0"
   },
   "source": [
    "# #TODO 0\n",
    "\n",
    "Convert the dataset to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:48:01.137832Z",
     "iopub.status.busy": "2025-02-02T13:48:01.137450Z",
     "iopub.status.idle": "2025-02-02T13:48:01.142684Z",
     "shell.execute_reply": "2025-02-02T13:48:01.141394Z",
     "shell.execute_reply.started": "2025-02-02T13:48:01.137802Z"
    },
    "id": "RQWm_iWBxFQ8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a lowercase dataset for uncased BERT\n",
    "def lower_case_sentences(examples):\n",
    "  lower_cased_examples = examples\n",
    "\n",
    "  # fill code here to lower case the \"sentence\" and \"label_tokens\"\n",
    "  lower_cased_examples['sentence'] = lower_cased_examples['sentence'].lower()\n",
    "  lower_cased_examples['label_tokens'] = [tok.lower() for tok in lower_cased_examples['label_tokens']]\n",
    "  return lower_cased_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:48:03.189314Z",
     "iopub.status.busy": "2025-02-02T13:48:03.188981Z",
     "iopub.status.idle": "2025-02-02T13:48:05.446312Z",
     "shell.execute_reply": "2025-02-02T13:48:05.445275Z",
     "shell.execute_reply.started": "2025-02-02T13:48:03.189289Z"
    },
    "id": "ndBIqEpWuqBP",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96e6dee76d64ccbabb1b593c407e08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a1bfbe39344c0b99c686cb3accb018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4625 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orchidl = orchid.map(lower_case_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:48:13.581957Z",
     "iopub.status.busy": "2025-02-02T13:48:13.581481Z",
     "iopub.status.idle": "2025-02-02T13:48:13.587551Z",
     "shell.execute_reply": "2025-02-02T13:48:13.586650Z",
     "shell.execute_reply.started": "2025-02-02T13:48:13.581930Z"
    },
    "id": "z8xpcCqTrqbc",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n",
       "        num_rows: 18500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n",
       "        num_rows: 4625\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orchidl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:48:15.648445Z",
     "iopub.status.busy": "2025-02-02T13:48:15.648113Z",
     "iopub.status.idle": "2025-02-02T13:48:15.655040Z",
     "shell.execute_reply": "2025-02-02T13:48:15.654047Z",
     "shell.execute_reply.started": "2025-02-02T13:48:15.648421Z"
    },
    "id": "ecpDHyTPv2py",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1899',\n",
       " 'label_tokens': ['โดย',\n",
       "  'พิจารณา',\n",
       "  'จาก',\n",
       "  'พจนานุกรม',\n",
       "  'ภาษา',\n",
       "  'คู่',\n",
       "  ' ',\n",
       "  '(',\n",
       "  'bilingual transfer dictionary',\n",
       "  ')'],\n",
       " 'pos_tags': [25, 39, 38, 26, 26, 5, 37, 37, 26, 37],\n",
       " 'sentence': 'โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orchidl[\"train\"][1899]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgV4ohz2xTY9"
   },
   "source": [
    "Now let's examine the labels again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:48:19.687910Z",
     "iopub.status.busy": "2025-02-02T13:48:19.687534Z",
     "iopub.status.idle": "2025-02-02T13:48:19.696441Z",
     "shell.execute_reply": "2025-02-02T13:48:19.695516Z",
     "shell.execute_reply.started": "2025-02-02T13:48:19.687885Z"
    },
    "id": "DoUDQzM7q265",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\n",
      "tokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', 'bi', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\n",
      "label tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n",
      "label pos : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n"
     ]
    }
   ],
   "source": [
    "example = orchidl[\"train\"][1899]\n",
    "print('sentence :', example[\"sentence\"])\n",
    "tokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print('tokens :',tokens)\n",
    "print('label tokens :', example[\"label_tokens\"])\n",
    "print('label pos :', example[\"pos_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:48:21.833794Z",
     "iopub.status.busy": "2025-02-02T13:48:21.833291Z",
     "iopub.status.idle": "2025-02-02T13:48:21.843588Z",
     "shell.execute_reply": "2025-02-02T13:48:21.842460Z",
     "shell.execute_reply.started": "2025-02-02T13:48:21.833753Z"
    },
    "id": "aEHgBeX7fQFt",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence : การประชุมทางวิชาการ ครั้งที่ 1\n",
      "tokens : ['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']\n",
      "label tokens : ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']\n",
      "label pos : [21, 39, 26, 26, 37, 4, 18]\n"
     ]
    }
   ],
   "source": [
    "example = orchidl[\"train\"][0]\n",
    "print('sentence :', example[\"sentence\"])\n",
    "tokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print('tokens :',tokens)\n",
    "print('label tokens :', example[\"label_tokens\"])\n",
    "print('label pos :', example[\"pos_tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dVcLxYbrl4E"
   },
   "source": [
    "In the example above, tokens refer to those tokenized using the pretrained tokenizer, while label tokens refer to tokens tokenized from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1inxbOYuBpB"
   },
   "source": [
    "**Do you see something?**\n",
    "\n",
    "Yes, the tokens from the two tokenizers do not match.\n",
    "\n",
    "- sentence : `การประชุมทางวิชาการ ครั้งที่ 1`\n",
    "\n",
    "---\n",
    "\n",
    "- tokens : `['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- label tokens : `['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']`\n",
    "- label pos : `[21, 39, 26, 26, 37, 4, 18]`\n",
    "\n",
    "You can see that in our label tokens, 'การ' has a POS tag of 21, and 'ประชุม' has a POS tag of 39. However, when we tokenize the sentence using WangchanBERTa, we get the token 'การประชุม'. What POS tag should we assign to this new token?\n",
    "\n",
    "**What should we do ?**\n",
    "\n",
    "Based on this example, we found that the tokens from the WangchanBERTa do not directly align with our label tokens. This means we cannot directly use the label POS tags. Therefore, we need to reassign POS tags to the tokens produced by WangchanBERTa tokenization. The method we will use is majority voting:\n",
    "- If a token from the WangchanBERTa matches a label token exactly, we will directly assign the POS tag from the label POS.\n",
    "- If the token generated overlaps or combines multiple label tokens, we assign the POS tag based on the number of characters in each token: If the token contains the most characters from any label token, we assign the POS tag from that label token.\n",
    "\n",
    "**Example :**\n",
    "\n",
    "    # \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n",
    "    # \"การ\" has a POS tag of 21,\n",
    "    # and \"ประชุม\" has a POS tag of 39.\n",
    "    # Therefore, the POS tag for \"การประชุม\" is 39,\n",
    "    # as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n",
    "\n",
    "    # 'ทางวิชาการ' (10 chars) is formed from 'ทาง' (3 chars) + 'วิชาการ' (7 chars)\n",
    "    # \"ทาง\" has a POS tag of 26,\n",
    "    # and \"วิชาการ\" has a POS tag of 2.\n",
    "    # Therefore, the POS tag for \"ทางวิชาการ\" is 2,\n",
    "    # as \"ทางวิชาการ\" is derived more from the \"ทาง\" part than from the \"วิชาการ\" part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTkgye8K8sd8"
   },
   "source": [
    "# #TODO 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgU8Nudh2rUJ"
   },
   "source": [
    "`**Warning: Please be careful of <unk>, an unknown word token.**`\n",
    "\n",
    "`**Warning: Please be careful of \" ำ \", the 'am' vowel. WangchanBERTa's internal preprocessing replaces all \" ำ \" to 'ํ' and 'า'**`\n",
    "\n",
    "Assigning the label -100 to the special tokens `[<s>]` and `[</s>]` and `[_]`  so they’re ignored by the PyTorch loss function (see [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html): ignore_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:48:25.819129Z",
     "iopub.status.busy": "2025-02-02T13:48:25.818816Z",
     "iopub.status.idle": "2025-02-02T13:48:25.829882Z",
     "shell.execute_reply": "2025-02-02T13:48:25.828703Z",
     "shell.execute_reply.started": "2025-02-02T13:48:25.819106Z"
    },
    "id": "bdxxiUU69lDx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def majority_vote_pos(examples):\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # TO DO: Since the tokens from the output of the pretrained tokenizer\n",
    "    # do not match the tokens in the label tokens of the dataset,\n",
    "    # the task is to create a function to determine the POS tags of the tokens generated by the pretrained tokenizer.\n",
    "    # This should be done by referencing the POS tags in the label tokens. If a token partially overlaps with others,\n",
    "    # the POS tag from the segment with the greater number of characters should be assigned.\n",
    "    #\n",
    "    # Example :\n",
    "    # \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n",
    "    # \"การ\" has a POS tag of 21,\n",
    "    # and \"ประชุม\" has a POS tag of 39.\n",
    "    # Therefore, the POS tag for \"การประชุม\" is 39,\n",
    "    # as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n",
    "    #\n",
    "    # 'ทางวิชาการ' (10 chars) is formed from 'ทาง' (3 chars) + 'วิชาการ' (7 chars)\n",
    "    # \"ทาง\" has a POS tag of 26,\n",
    "    # and \"วิชาการ\" has a POS tag of 2.\n",
    "    # Therefore, the POS tag for \"ทางวิชาการ\" is 2,\n",
    "    # as \"ทางวิชาการ\" is derived more from the \"ทาง\" part than from the \"วิชาการ\" part.\n",
    "\n",
    "    # tokenize word by pretrained tokenizer\n",
    "    tokenized_inputs = tokenizer([examples[\"sentence\"]], is_split_into_words=True)\n",
    "\n",
    "    # FILL CODE HERE\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n",
    "    \n",
    "    # Get the label tokens and POS tags from the example\n",
    "    label_tokens = examples[\"label_tokens\"]\n",
    "    pos_tags = examples[\"pos_tags\"]\n",
    "    \n",
    "    # Original text from label tokens\n",
    "    original_text = ''.join(label_tokens)\n",
    "    \n",
    "    # Create spans for original tokens with their POS tags\n",
    "    original_spans = []\n",
    "    current_pos = 0\n",
    "    for token, pos in zip(label_tokens, pos_tags):\n",
    "        end = current_pos + len(token)\n",
    "        original_spans.append((current_pos, end, pos))\n",
    "        current_pos = end\n",
    "    \n",
    "    # Special character replacements\n",
    "    special_to_char = {\n",
    "        '▁': '',\n",
    "        '<s>': '',\n",
    "        '</s>': ''\n",
    "    }\n",
    "    \n",
    "    # Map POS tags to new tokens\n",
    "    new_pos_result = []\n",
    "    current_pos = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Handle special tokens\n",
    "        if token == '<_>':\n",
    "            token = ' '\n",
    "        # Remove special prefix if present\n",
    "        if len(token) > 1 and token[0] == '▁':\n",
    "            token = token[1:]\n",
    "        # Handle special character replacements\n",
    "        token = token.replace('ํา', \"ำ\")\n",
    "        converted_token = special_to_char.get(token, token)\n",
    "        token_len = len(converted_token)\n",
    "        \n",
    "        # If the token is empty, append None and continue\n",
    "        if token_len == 0:\n",
    "            new_pos_result.append(None)\n",
    "            continue\n",
    "        \n",
    "        # Check if the current position + token length exceeds the original text length\n",
    "        if current_pos + token_len > len(original_text):\n",
    "            new_pos_result.append(None)\n",
    "            current_pos += token_len  # Advance to avoid infinite loops\n",
    "            continue\n",
    "        \n",
    "        # Check if the expected substring matches the converted token\n",
    "        expected_substring = original_text[current_pos:current_pos + token_len]\n",
    "        if expected_substring != converted_token:\n",
    "            new_pos_result.append(None)\n",
    "            current_pos += token_len\n",
    "            continue\n",
    "        \n",
    "        # Calculate the start and end positions of the current token\n",
    "        start = current_pos\n",
    "        end = current_pos + token_len\n",
    "        \n",
    "        # Find overlapping spans and their contributions\n",
    "        contributions = []\n",
    "        for (tok_start, tok_end, pos) in original_spans:\n",
    "            overlap_start = max(start, tok_start)\n",
    "            overlap_end = min(end, tok_end)\n",
    "            if overlap_start < overlap_end:\n",
    "                overlap_length = overlap_end - overlap_start\n",
    "                contributions.append((overlap_length, pos, tok_start))\n",
    "        \n",
    "        # If no contributions, append None\n",
    "        if not contributions:\n",
    "            new_pos_result.append(None)\n",
    "        else:\n",
    "            # Sort contributions by overlap length and start position\n",
    "            contributions.sort(key=lambda x: (-x[0], -x[2]))\n",
    "            selected_pos = contributions[0][1]  # Select the POS tag with the largest overlap\n",
    "            new_pos_result.append(selected_pos)\n",
    "        \n",
    "        # Advance the current position\n",
    "        current_pos = end\n",
    "    \n",
    "    # Convert None to -100 for special tokens and mismatches\n",
    "    new_pos_result = [-100 if tag is None else tag for tag in new_pos_result]\n",
    "    \n",
    "    # Update the tokenized_inputs dictionary with the new tokens and their corresponding POS tags\n",
    "    tokenized_inputs['tokens'] = tokens\n",
    "    tokenized_inputs['labels'] = new_pos_result\n",
    "    \n",
    "    return tokenized_inputs\n",
    "    ####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:48:30.145629Z",
     "iopub.status.busy": "2025-02-02T13:48:30.145250Z",
     "iopub.status.idle": "2025-02-02T13:48:45.900823Z",
     "shell.execute_reply": "2025-02-02T13:48:45.899775Z",
     "shell.execute_reply.started": "2025-02-02T13:48:30.145600Z"
    },
    "id": "doFKOhpbGf9N",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04ee74e68b64d12b01b9eb3bcef737a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53956666d1646edb1636a3443ec7cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4625 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_orchid = orchidl.map(majority_vote_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:48:45.902262Z",
     "iopub.status.busy": "2025-02-02T13:48:45.901979Z",
     "iopub.status.idle": "2025-02-02T13:48:45.907647Z",
     "shell.execute_reply": "2025-02-02T13:48:45.906656Z",
     "shell.execute_reply.started": "2025-02-02T13:48:45.902238Z"
    },
    "id": "uvdDnWeOJYpv",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label_tokens', 'pos_tags', 'sentence', 'input_ids', 'attention_mask', 'tokens', 'labels'],\n",
       "        num_rows: 18500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'label_tokens', 'pos_tags', 'sentence', 'input_ids', 'attention_mask', 'tokens', 'labels'],\n",
       "        num_rows: 4625\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_orchid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:48:45.909498Z",
     "iopub.status.busy": "2025-02-02T13:48:45.909240Z",
     "iopub.status.idle": "2025-02-02T13:48:45.929049Z",
     "shell.execute_reply": "2025-02-02T13:48:45.928001Z",
     "shell.execute_reply.started": "2025-02-02T13:48:45.909476Z"
    },
    "id": "ojrRF85dJbwf",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'label_tokens': ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1'],\n",
       " 'pos_tags': [21, 39, 26, 26, 37, 4, 18],\n",
       " 'sentence': 'การประชุมทางวิชาการ ครั้งที่ 1',\n",
       " 'input_ids': [5, 10, 882, 8222, 8, 10, 1014, 8, 10, 59, 6],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'tokens': ['<s>',\n",
       "  '▁',\n",
       "  'การประชุม',\n",
       "  'ทางวิชาการ',\n",
       "  '<_>',\n",
       "  '▁',\n",
       "  'ครั้งที่',\n",
       "  '<_>',\n",
       "  '▁',\n",
       "  '1',\n",
       "  '</s>'],\n",
       " 'labels': [-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_orchid['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:48:45.930522Z",
     "iopub.status.busy": "2025-02-02T13:48:45.930180Z",
     "iopub.status.idle": "2025-02-02T13:48:45.946067Z",
     "shell.execute_reply": "2025-02-02T13:48:45.945159Z",
     "shell.execute_reply.started": "2025-02-02T13:48:45.930489Z"
    },
    "id": "KMfzFnjSdCGI",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 0\n",
      "label_tokens : ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']\n",
      "pos_tags : [21, 39, 26, 26, 37, 4, 18]\n",
      "sentence : การประชุมทางวิชาการ ครั้งที่ 1\n",
      "input_ids : [5, 10, 882, 8222, 8, 10, 1014, 8, 10, 59, 6]\n",
      "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "tokens : ['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']\n",
      "labels : [-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]\n"
     ]
    }
   ],
   "source": [
    "example = tokenized_orchid[\"train\"][0]\n",
    "for i in example :\n",
    "    print(i, \":\", example[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lhsQcdL6H3J"
   },
   "source": [
    "This is the result after we realigned the POS based on the majority vote.\n",
    "- label_tokens : `['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']`\n",
    "- pos_tags : `[21, 39, 26, 26, 37, 4, 18]`\n",
    "- tokens : `['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']`\n",
    "- labels : `[-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]`\n",
    "\n",
    "`['<s>', '▁', '</s>'] : -100`\n",
    "\n",
    "**Check :**\n",
    "\n",
    "> \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n",
    "\n",
    "\n",
    "> \"การ\" has a POS tag of 21,\n",
    "\n",
    "> and \"ประชุม\" has a POS tag of 39.\n",
    "\n",
    "> Therefore, the POS tag for \"การประชุม\" is 39,\n",
    "\n",
    "> as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:48:54.599338Z",
     "iopub.status.busy": "2025-02-02T13:48:54.598976Z",
     "iopub.status.idle": "2025-02-02T13:48:54.610151Z",
     "shell.execute_reply": "2025-02-02T13:48:54.609184Z",
     "shell.execute_reply.started": "2025-02-02T13:48:54.599304Z"
    },
    "id": "iOE5CEgZdO9c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 1899\n",
      "label_tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n",
      "pos_tags : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n",
      "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\n",
      "input_ids : [5, 489, 15617, 19737, 958, 493, 8, 1241, 4906, 11608, 12177, 8, 10, 11392, 9806, 8, 10, 2951, 15779, 8001, 29, 6]\n",
      "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "tokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', 'bi', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\n",
      "labels : [-100, 25, 39, 26, 26, 5, 37, 37, 26, 26, 26, 26, -100, 26, 26, 26, -100, 26, 26, 26, 37, -100]\n"
     ]
    }
   ],
   "source": [
    "# hard test case\n",
    "example = tokenized_orchid[\"train\"][1899]\n",
    "for i in example :\n",
    "    print(i, \":\", example[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv_NkVQ6qsJe"
   },
   "source": [
    "Expected output\n",
    "\n",
    "\n",
    "```\n",
    "id : 1899\n",
    "label_tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n",
    "pos_tags : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n",
    "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\n",
    "input_ids : [5, 489, 15617, 19737, 958, 493, 8, 1241, 4906, 11608, 12177, 8, 10, 11392, 9806, 8, 10, 2951, 15779, 8001, 29, 6]\n",
    "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "tokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', 'bi', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\n",
    "labels : [-100, 25, 39, 26, 26, 5, 37, 37, 26, 26, 26, 26, -100, 26, 26, 26, -100, 26, 26, 26, 37, -100]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pwADd1a85bn"
   },
   "source": [
    "# Train and Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsnlIUJvYEy2"
   },
   "source": [
    "We will create a batch of examples using [DataCollatorWithPadding.](https://huggingface.co/docs/transformers/v4.48.0/en/main_classes/data_collator#transformers.DataCollatorWithPadding)  \n",
    "\n",
    "Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset.\n",
    "\n",
    "DataCollatorWithPadding will help us pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length. This allows for efficient computation during each batch.\n",
    "\n",
    "*   DataCollatorForTokenClassification : `padding (bool, str or PaddingStrategy, optional, defaults to True)`\n",
    "*   `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single sequence is provided).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:49:01.800456Z",
     "iopub.status.busy": "2025-02-02T13:49:01.800105Z",
     "iopub.status.idle": "2025-02-02T13:49:01.804706Z",
     "shell.execute_reply": "2025-02-02T13:49:01.803644Z",
     "shell.execute_reply.started": "2025-02-02T13:49:01.800425Z"
    },
    "id": "CcAY4-E2J6e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg4v14KcElbY"
   },
   "source": [
    "For evaluating your model’s performance. You can quickly load a evaluation method with the [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [seqeval](https://huggingface.co/spaces/evaluate-metric/seqeval) framework (see the Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric). Seqeval actually produces several scores: precision, recall, F1, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:49:04.567568Z",
     "iopub.status.busy": "2025-02-02T13:49:04.567200Z",
     "iopub.status.idle": "2025-02-02T13:49:04.945228Z",
     "shell.execute_reply": "2025-02-02T13:49:04.944493Z",
     "shell.execute_reply.started": "2025-02-02T13:49:04.567541Z"
    },
    "id": "cZk3PjndK-Q8",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff54348603544c8b97c08a9909af7b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FllGDNO5RUIA"
   },
   "source": [
    "Huggingface requires us to write a ``compute_metrics()`` function. This will be invoked when huggingface evalutes a model.\n",
    "\n",
    "Note that we ignore to evaluate on -100 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:49:07.635443Z",
     "iopub.status.busy": "2025-02-02T13:49:07.635126Z",
     "iopub.status.idle": "2025-02-02T13:49:07.641451Z",
     "shell.execute_reply": "2025-02-02T13:49:07.640606Z",
     "shell.execute_reply.started": "2025-02-02T13:49:07.635417Z"
    },
    "id": "vDwNPItNLTM1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kt13vTldvTw4"
   },
   "source": [
    "The total number of labels in our POS tag set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:49:10.141561Z",
     "iopub.status.busy": "2025-02-02T13:49:10.141230Z",
     "iopub.status.idle": "2025-02-02T13:49:10.151722Z",
     "shell.execute_reply": "2025-02-02T13:49:10.150859Z",
     "shell.execute_reply.started": "2025-02-02T13:49:10.141535Z"
    },
    "id": "JD84B79-Lxwf",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADVI': 0,\n",
       " 'ADVN': 1,\n",
       " 'ADVP': 2,\n",
       " 'ADVS': 3,\n",
       " 'CFQC': 4,\n",
       " 'CLTV': 5,\n",
       " 'CMTR': 6,\n",
       " 'CMTR@PUNC': 7,\n",
       " 'CNIT': 8,\n",
       " 'CVBL': 9,\n",
       " 'DCNM': 10,\n",
       " 'DDAC': 11,\n",
       " 'DDAN': 12,\n",
       " 'DDAQ': 13,\n",
       " 'DDBQ': 14,\n",
       " 'DIAC': 15,\n",
       " 'DIAQ': 16,\n",
       " 'DIBQ': 17,\n",
       " 'DONM': 18,\n",
       " 'EAFF': 19,\n",
       " 'EITT': 20,\n",
       " 'FIXN': 21,\n",
       " 'FIXV': 22,\n",
       " 'JCMP': 23,\n",
       " 'JCRG': 24,\n",
       " 'JSBR': 25,\n",
       " 'NCMN': 26,\n",
       " 'NCNM': 27,\n",
       " 'NEG': 28,\n",
       " 'NLBL': 29,\n",
       " 'NONM': 30,\n",
       " 'NPRP': 31,\n",
       " 'NTTL': 32,\n",
       " 'PDMN': 33,\n",
       " 'PNTR': 34,\n",
       " 'PPRS': 35,\n",
       " 'PREL': 36,\n",
       " 'PUNC': 37,\n",
       " 'RPRE': 38,\n",
       " 'VACT': 39,\n",
       " 'VATT': 40,\n",
       " 'VSTA': 41,\n",
       " 'XVAE': 42,\n",
       " 'XVAM': 43,\n",
       " 'XVBB': 44,\n",
       " 'XVBM': 45,\n",
       " 'XVMM': 46}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {\n",
    "    0: 'ADVI',\n",
    "    1: 'ADVN',\n",
    "    2: 'ADVP',\n",
    "    3: 'ADVS',\n",
    "    4: 'CFQC',\n",
    "    5: 'CLTV',\n",
    "    6: 'CMTR',\n",
    "    7: 'CMTR@PUNC',\n",
    "    8: 'CNIT',\n",
    "    9: 'CVBL',\n",
    "    10: 'DCNM',\n",
    "    11: 'DDAC',\n",
    "    12: 'DDAN',\n",
    "    13: 'DDAQ',\n",
    "    14: 'DDBQ',\n",
    "    15: 'DIAC',\n",
    "    16: 'DIAQ',\n",
    "    17: 'DIBQ',\n",
    "    18: 'DONM',\n",
    "    19: 'EAFF',\n",
    "    20: 'EITT',\n",
    "    21: 'FIXN',\n",
    "    22: 'FIXV',\n",
    "    23: 'JCMP',\n",
    "    24: 'JCRG',\n",
    "    25: 'JSBR',\n",
    "    26: 'NCMN',\n",
    "    27: 'NCNM',\n",
    "    28: 'NEG',\n",
    "    29: 'NLBL',\n",
    "    30: 'NONM',\n",
    "    31: 'NPRP',\n",
    "    32: 'NTTL',\n",
    "    33: 'PDMN',\n",
    "    34: 'PNTR',\n",
    "    35: 'PPRS',\n",
    "    36: 'PREL',\n",
    "    37: 'PUNC',\n",
    "    38: 'RPRE',\n",
    "    39: 'VACT',\n",
    "    40: 'VATT',\n",
    "    41: 'VSTA',\n",
    "    42: 'XVAE',\n",
    "    43: 'XVAM',\n",
    "    44: 'XVBB',\n",
    "    45: 'XVBM',\n",
    "    46: 'XVMM',\n",
    "    # 47: 'O'\n",
    "}\n",
    "label2id = {}\n",
    "for k, v in id2label.items() :\n",
    "    label2id[v] = k\n",
    "\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:49:16.554153Z",
     "iopub.status.busy": "2025-02-02T13:49:16.553787Z",
     "iopub.status.idle": "2025-02-02T13:49:16.560199Z",
     "shell.execute_reply": "2025-02-02T13:49:16.559251Z",
     "shell.execute_reply.started": "2025-02-02T13:49:16.554122Z"
    },
    "id": "mQtGN8QQQLME",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADVI',\n",
       " 'ADVN',\n",
       " 'ADVP',\n",
       " 'ADVS',\n",
       " 'CFQC',\n",
       " 'CLTV',\n",
       " 'CMTR',\n",
       " 'CMTR@PUNC',\n",
       " 'CNIT',\n",
       " 'CVBL',\n",
       " 'DCNM',\n",
       " 'DDAC',\n",
       " 'DDAN',\n",
       " 'DDAQ',\n",
       " 'DDBQ',\n",
       " 'DIAC',\n",
       " 'DIAQ',\n",
       " 'DIBQ',\n",
       " 'DONM',\n",
       " 'EAFF',\n",
       " 'EITT',\n",
       " 'FIXN',\n",
       " 'FIXV',\n",
       " 'JCMP',\n",
       " 'JCRG',\n",
       " 'JSBR',\n",
       " 'NCMN',\n",
       " 'NCNM',\n",
       " 'NEG',\n",
       " 'NLBL',\n",
       " 'NONM',\n",
       " 'NPRP',\n",
       " 'NTTL',\n",
       " 'PDMN',\n",
       " 'PNTR',\n",
       " 'PPRS',\n",
       " 'PREL',\n",
       " 'PUNC',\n",
       " 'RPRE',\n",
       " 'VACT',\n",
       " 'VATT',\n",
       " 'VSTA',\n",
       " 'XVAE',\n",
       " 'XVAM',\n",
       " 'XVBB',\n",
       " 'XVBM',\n",
       " 'XVMM']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [i for i in id2label.values()]\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nu7Z3QH_BJe4"
   },
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBYlcF-gDZcF"
   },
   "source": [
    "Select a pretrained model for fine-tuning to develop a POS Tagger model using the Orchid corpus dataset.\n",
    "\n",
    "\n",
    "\n",
    "*   model : `wangchanberta-base-att-spm-uncased`\n",
    "*   Don't forget to update the num_labels.\n",
    "\n",
    "You’re ready to start training your model now! Load pretrained model with AutoModelForTokenClassification along with the number of expected labels, and the label mappings:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OOu8s-mO_Fw"
   },
   "source": [
    "`In the first part, we require you to select the wangchanberta-base-att-spm-uncased.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:49:20.951648Z",
     "iopub.status.busy": "2025-02-02T13:49:20.951209Z",
     "iopub.status.idle": "2025-02-02T13:49:24.460424Z",
     "shell.execute_reply": "2025-02-02T13:49:24.459666Z",
     "shell.execute_reply.started": "2025-02-02T13:49:20.951603Z"
    },
    "id": "OOsnubHyDMmA",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775b8ac3964e48029874279efbab1095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/423M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForTokenClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_names = [\n",
    "    'wangchanberta-base-att-spm-uncased',\n",
    "    'wangchanberta-base-wiki-newmm',\n",
    "    'wangchanberta-base-wiki-ssg',\n",
    "    'wangchanberta-base-wiki-sefr',\n",
    "    'wangchanberta-base-wiki-spm',\n",
    "]\n",
    "\n",
    "#@title Choose Pretrained Model\n",
    "model_name = \"wangchanberta-base-att-spm-uncased\"\n",
    "\n",
    "#create model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    f\"airesearch/{model_name}\",\n",
    "    revision='main',\n",
    "    num_labels=47, id2label=id2label, label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H2OExQrCAfX"
   },
   "source": [
    "### #TODO 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBZKrz8nFXyT"
   },
   "source": [
    "* Configure your training hyperparameters using `**TrainingArguments**`. The only required parameter is is `output_dir`, which determines the directory where your model will be saved. To upload the model to the Hugging Face Hub, set push_to_hub=True (note: you must be logged into Hugging Face for this). During training, the Trainer will compute seqeval metrics at the end of each epoch and store the training checkpoint.\n",
    "* Provide the `**Trainer**` with the training arguments, as well as the model, dataset, tokenizer, data collator, and compute_metrics function.\n",
    "* Use `**train()**` to fine-tune the model.\n",
    "\n",
    "\n",
    "Read [huggingface's tutorial](https://huggingface.co/docs/transformers/en/tasks/token_classification) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:49:26.986044Z",
     "iopub.status.busy": "2025-02-02T13:49:26.985258Z",
     "iopub.status.idle": "2025-02-02T13:58:59.245710Z",
     "shell.execute_reply": "2025-02-02T13:58:59.244889Z",
     "shell.execute_reply.started": "2025-02-02T13:49:26.986005Z"
    },
    "id": "gMUkNHNrCwsl",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250202_134930-f5zflfgn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/demonstem/huggingface/runs/f5zflfgn' target=\"_blank\">revived-eon-3</a></strong> to <a href='https://wandb.ai/demonstem/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/demonstem/huggingface' target=\"_blank\">https://wandb.ai/demonstem/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/demonstem/huggingface/runs/f5zflfgn' target=\"_blank\">https://wandb.ai/demonstem/huggingface/runs/f5zflfgn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1737' max='1737' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1737/1737 09:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.369600</td>\n",
       "      <td>0.329346</td>\n",
       "      <td>0.849560</td>\n",
       "      <td>0.856927</td>\n",
       "      <td>0.853228</td>\n",
       "      <td>0.909372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.290324</td>\n",
       "      <td>0.861986</td>\n",
       "      <td>0.869443</td>\n",
       "      <td>0.865698</td>\n",
       "      <td>0.916770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.263900</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.863540</td>\n",
       "      <td>0.871621</td>\n",
       "      <td>0.867562</td>\n",
       "      <td>0.916296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2254: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1737, training_loss=0.4446723394580593, metrics={'train_runtime': 569.0163, 'train_samples_per_second': 97.537, 'train_steps_per_second': 3.053, 'total_flos': 1816702276832832.0, 'train_loss': 0.4446723394580593, 'epoch': 3.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    #########################\n",
    "    output_dir=\"./results\",  # Directory where the model checkpoints and logs will be saved\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    learning_rate=2e-5,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=16,  # Batch size per GPU/CPU for training\n",
    "    per_device_eval_batch_size=16,  # Batch size per GPU/CPU for evaluation\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Strength of weight decay\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    save_total_limit=2,  # Limit the number of saved checkpoints\n",
    "    push_to_hub=False,  # Set to True if you want to upload the model to the Hugging Face Hub\n",
    "    logging_dir=\"./logs\",  # Directory for storing logs\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to determine the best model\n",
    "    greater_is_better=False,  # Lower eval_loss is better\n",
    "    #########################\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    #########################\n",
    "    model=model,  # The pre-trained model\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=tokenized_orchid['train'],  # Training dataset\n",
    "    eval_dataset=tokenized_orchid['test'],  # Evaluation dataset\n",
    "    tokenizer=tokenizer,  # Tokenizer\n",
    "    data_collator=data_collator,  # Data collator\n",
    "    compute_metrics=compute_metrics,  # Function to compute metrics\n",
    "    ########################\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiXUG6aakihd"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OPVknoQk4wL"
   },
   "source": [
    "With your model fine-tuned, you can now perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:59:05.285805Z",
     "iopub.status.busy": "2025-02-02T13:59:05.285443Z",
     "iopub.status.idle": "2025-02-02T13:59:05.290285Z",
     "shell.execute_reply": "2025-02-02T13:59:05.289515Z",
     "shell.execute_reply.started": "2025-02-02T13:59:05.285775Z"
    },
    "id": "mMyq6I9CkZ1R",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = \"การประชุมทางวิชาการ ครั้งที่ 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FL5qxD3EPLFt"
   },
   "source": [
    "`In the first part, we require you to select the wangchanberta-base-att-spm-uncased.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:59:16.633818Z",
     "iopub.status.busy": "2025-02-02T13:59:16.633457Z",
     "iopub.status.idle": "2025-02-02T13:59:16.932742Z",
     "shell.execute_reply": "2025-02-02T13:59:16.932088Z",
     "shell.execute_reply.started": "2025-02-02T13:59:16.633789Z"
    },
    "id": "sKgM-EaGfxA4",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
      "The class this function is called from is 'WangchanbertaTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
      "The class this function is called from is 'WangchanbertaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load pretrained tokenizer from Hugging Face\n",
    "#@title Choose Pretrained Model\n",
    "model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "\n",
    "tokenizer = Tokenizer(model_name).from_pretrained(model_name)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:59:20.555923Z",
     "iopub.status.busy": "2025-02-02T13:59:20.555459Z",
     "iopub.status.idle": "2025-02-02T13:59:20.567634Z",
     "shell.execute_reply": "2025-02-02T13:59:20.566690Z",
     "shell.execute_reply.started": "2025-02-02T13:59:20.555880Z"
    },
    "id": "PcRf-Q9nf4-t",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   5,   10,  882, 8222,    8,   10, 1014,    8,   10,   59,    6]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:00:57.951807Z",
     "iopub.status.busy": "2025-02-02T14:00:57.951323Z",
     "iopub.status.idle": "2025-02-02T14:00:59.364513Z",
     "shell.execute_reply": "2025-02-02T14:00:59.363611Z",
     "shell.execute_reply.started": "2025-02-02T14:00:57.951771Z"
    },
    "id": "6ADY5OuqkkHb",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "## Load your fine-tuned model from Hugging Face\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"results/checkpoint-1737\") ## your model path from Hugging Face\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:01:04.263921Z",
     "iopub.status.busy": "2025-02-02T14:01:04.263431Z",
     "iopub.status.idle": "2025-02-02T14:01:04.272051Z",
     "shell.execute_reply": "2025-02-02T14:01:04.271119Z",
     "shell.execute_reply.started": "2025-02-02T14:01:04.263880Z"
    },
    "id": "AACsd7VZgT1E",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PUNC',\n",
       " 'PUNC',\n",
       " 'VACT',\n",
       " 'NCMN',\n",
       " 'PUNC',\n",
       " 'PUNC',\n",
       " 'CFQC',\n",
       " 'DONM',\n",
       " 'DONM',\n",
       " 'DONM',\n",
       " 'PUNC']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = torch.argmax(logits, dim=2)\n",
    "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
    "predicted_token_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:01:07.212543Z",
     "iopub.status.busy": "2025-02-02T14:01:07.212121Z",
     "iopub.status.idle": "2025-02-02T14:01:07.220667Z",
     "shell.execute_reply": "2025-02-02T14:01:07.219760Z",
     "shell.execute_reply.started": "2025-02-02T14:01:07.212498Z"
    },
    "id": "kJKGbf4Rk0c9",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'ADVI',\n",
       " 1: 'ADVN',\n",
       " 2: 'ADVP',\n",
       " 3: 'ADVS',\n",
       " 4: 'CFQC',\n",
       " 5: 'CLTV',\n",
       " 6: 'CMTR',\n",
       " 7: 'CMTR@PUNC',\n",
       " 8: 'CNIT',\n",
       " 9: 'CVBL',\n",
       " 10: 'DCNM',\n",
       " 11: 'DDAC',\n",
       " 12: 'DDAN',\n",
       " 13: 'DDAQ',\n",
       " 14: 'DDBQ',\n",
       " 15: 'DIAC',\n",
       " 16: 'DIAQ',\n",
       " 17: 'DIBQ',\n",
       " 18: 'DONM',\n",
       " 19: 'EAFF',\n",
       " 20: 'EITT',\n",
       " 21: 'FIXN',\n",
       " 22: 'FIXV',\n",
       " 23: 'JCMP',\n",
       " 24: 'JCRG',\n",
       " 25: 'JSBR',\n",
       " 26: 'NCMN',\n",
       " 27: 'NCNM',\n",
       " 28: 'NEG',\n",
       " 29: 'NLBL',\n",
       " 30: 'NONM',\n",
       " 31: 'NPRP',\n",
       " 32: 'NTTL',\n",
       " 33: 'PDMN',\n",
       " 34: 'PNTR',\n",
       " 35: 'PPRS',\n",
       " 36: 'PREL',\n",
       " 37: 'PUNC',\n",
       " 38: 'RPRE',\n",
       " 39: 'VACT',\n",
       " 40: 'VATT',\n",
       " 41: 'VSTA',\n",
       " 42: 'XVAE',\n",
       " 43: 'XVAM',\n",
       " 44: 'XVBB',\n",
       " 45: 'XVBM',\n",
       " 46: 'XVMM'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:01:19.468761Z",
     "iopub.status.busy": "2025-02-02T14:01:19.468394Z",
     "iopub.status.idle": "2025-02-02T14:01:19.548399Z",
     "shell.execute_reply": "2025-02-02T14:01:19.547362Z",
     "shell.execute_reply.started": "2025-02-02T14:01:19.468729Z"
    },
    "id": "6OikBqcKTcUY",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens : ['<s>', '▁', 'จะว่าไป', 'แล้ว', 'เชิง', 'เทียน', 'ของ', 'ผมก็', 'สวยดี', 'เหมือนกัน', '</s>']\n",
      "predict pos : ['PUNC', 'PUNC', 'JSBR', 'XVAE', 'NCMN', 'NCMN', 'RPRE', 'PPRS', 'VATT', 'ADVN', 'PUNC']\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "# ignore special tokens\n",
    "text = 'จะว่าไปแล้วเชิงเทียนของผมก็สวยดีเหมือนกัน'\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "tokenized_input = tokenizer([text], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print('tokens :', tokens)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
    "print('predict pos :', predicted_token_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-TtwYTCXs2O"
   },
   "source": [
    "**Evaluate model :**\n",
    "\n",
    "The output from the model is a softmax over classes. We choose the maximum class as the answer for evaluation. Again, we will ignore the -100 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:01:27.036810Z",
     "iopub.status.busy": "2025-02-02T14:01:27.036437Z",
     "iopub.status.idle": "2025-02-02T14:01:27.046812Z",
     "shell.execute_reply": "2025-02-02T14:01:27.046115Z",
     "shell.execute_reply.started": "2025-02-02T14:01:27.036780Z"
    },
    "id": "Hm_adLrcXyOe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def evaluation_report(y_true, y_pred, get_only_acc=False):\n",
    "    # retrieve all tags in y_true\n",
    "    tag_set = set()\n",
    "    for sent in y_true:\n",
    "        for tag in sent:\n",
    "            tag_set.add(tag)\n",
    "    for sent in y_pred:\n",
    "        for tag in sent:\n",
    "            tag_set.add(tag)\n",
    "    tag_list = sorted(list(tag_set))\n",
    "\n",
    "    # count correct points\n",
    "    tag_info = dict()\n",
    "    for tag in tag_list:\n",
    "        tag_info[tag] = {'correct_tagged': 0, 'y_true': 0, 'y_pred': 0}\n",
    "\n",
    "    all_correct = 0\n",
    "    all_count = sum([len(sent) for sent in y_true])\n",
    "    speacial_tag = 0\n",
    "    for sent_true, sent_pred in zip(y_true, y_pred):\n",
    "        for tag_true, tag_pred in zip(sent_true, sent_pred):\n",
    "            # pass special token\n",
    "            if tag_true == -100 :\n",
    "                speacial_tag += 1\n",
    "                pass\n",
    "            if tag_true == tag_pred:\n",
    "                tag_info[tag_true]['correct_tagged'] += 1\n",
    "                all_correct += 1\n",
    "            tag_info[tag_true]['y_true'] += 1\n",
    "            tag_info[tag_pred]['y_pred'] += 1\n",
    "    print('speacial_tag :',speacial_tag) # delete number of special token from all_count\n",
    "    accuracy = (all_correct / (all_count-speacial_tag))\n",
    "\n",
    "    # get only accuracy for testing\n",
    "    if get_only_acc:\n",
    "      return accuracy\n",
    "\n",
    "    accuracy *= 100\n",
    "\n",
    "\n",
    "    # summarize and make evaluation result\n",
    "    eval_list = list()\n",
    "    for tag in tag_list:\n",
    "        eval_result = dict()\n",
    "        eval_result['tag'] = tag\n",
    "        eval_result['correct_count'] = tag_info[tag]['correct_tagged']\n",
    "        precision = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_pred'])*100 if tag_info[tag]['y_pred'] else '-'\n",
    "        recall = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_true'])*100 if (tag_info[tag]['y_true'] > 0) else 0\n",
    "        eval_result['precision'] = precision\n",
    "        eval_result['recall'] = recall\n",
    "        eval_result['f1_score'] = (2*precision*recall)/(precision+recall) if (type(precision) is float and recall > 0) else '-'\n",
    "\n",
    "        eval_list.append(eval_result)\n",
    "\n",
    "    eval_list.append({'tag': 'accuracy=%.2f' % accuracy, 'correct_count': '', 'precision': '', 'recall': '', 'f1_score': ''})\n",
    "\n",
    "    df = pd.DataFrame.from_dict(eval_list)\n",
    "    df = df[['tag', 'precision', 'recall', 'f1_score', 'correct_count']]\n",
    "\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:01:29.961993Z",
     "iopub.status.busy": "2025-02-02T14:01:29.961606Z",
     "iopub.status.idle": "2025-02-02T14:01:29.966863Z",
     "shell.execute_reply": "2025-02-02T14:01:29.966046Z",
     "shell.execute_reply.started": "2025-02-02T14:01:29.961961Z"
    },
    "id": "x7Lryj2aYCdn",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# prepare test set\n",
    "test_data = tokenized_orchid[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:01:33.408362Z",
     "iopub.status.busy": "2025-02-02T14:01:33.408049Z",
     "iopub.status.idle": "2025-02-02T14:01:34.090241Z",
     "shell.execute_reply": "2025-02-02T14:01:34.089592Z",
     "shell.execute_reply.started": "2025-02-02T14:01:33.408338Z"
    },
    "id": "FGXWNs9RY2Zv",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# labels for test set\n",
    "y_test = []\n",
    "for inputs in test_data:\n",
    "  y_test.append(inputs['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:07:04.931916Z",
     "iopub.status.busy": "2025-02-02T14:07:04.931531Z",
     "iopub.status.idle": "2025-02-02T14:12:03.421890Z",
     "shell.execute_reply": "2025-02-02T14:12:03.420841Z",
     "shell.execute_reply.started": "2025-02-02T14:07:04.931888Z"
    },
    "id": "U6__09qnX1DW",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd24b7086424392b54872134fb26676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = []\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "for _, inputs in enumerate(tqdm(test_data)):\n",
    "    text = inputs['sentence']\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        pred = model(**inputs).logits\n",
    "        predictions = torch.argmax(pred, dim=2)\n",
    "        # Append padded predictions to y_pred\n",
    "        y_pred.append(predictions.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:12:03.423358Z",
     "iopub.status.busy": "2025-02-02T14:12:03.423137Z",
     "iopub.status.idle": "2025-02-02T14:12:03.429707Z",
     "shell.execute_reply": "2025-02-02T14:12:03.428836Z",
     "shell.execute_reply.started": "2025-02-02T14:12:03.423338Z"
    },
    "id": "yX0BhOe7g3eh",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 37, 39, 26, 26, 26, 37, 37, 26, 26, 26, 41, 37, 37, 26, 26, 39, 26, 37]\n",
      "[-100, 37, 39, 26, 26, 26, 37, -100, 26, 26, 26, 41, 37, -100, 26, 26, 39, 26, -100]\n"
     ]
    }
   ],
   "source": [
    "# check our prediction with label\n",
    "# -100 is special tokens : [<s>, </s>, _]\n",
    "print(y_pred[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:12:03.431344Z",
     "iopub.status.busy": "2025-02-02T14:12:03.431016Z",
     "iopub.status.idle": "2025-02-02T14:12:03.522778Z",
     "shell.execute_reply": "2025-02-02T14:12:03.522107Z",
     "shell.execute_reply.started": "2025-02-02T14:12:03.431310Z"
    },
    "id": "Na0L6NUdgLaE",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speacial_tag : 21039\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>30.769231</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>78.655283</td>\n",
       "      <td>70.123692</td>\n",
       "      <td>74.144869</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>22.413793</td>\n",
       "      <td>30.952381</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>88.333333</td>\n",
       "      <td>86.885246</td>\n",
       "      <td>87.603306</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>69.444444</td>\n",
       "      <td>57.803468</td>\n",
       "      <td>63.091483</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>81.533101</td>\n",
       "      <td>98.319328</td>\n",
       "      <td>89.142857</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>63.225806</td>\n",
       "      <td>76.165803</td>\n",
       "      <td>69.095182</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>93.774704</td>\n",
       "      <td>90.209125</td>\n",
       "      <td>91.957364</td>\n",
       "      <td>949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>94.402036</td>\n",
       "      <td>81.359649</td>\n",
       "      <td>87.396938</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>52.095808</td>\n",
       "      <td>83.653846</td>\n",
       "      <td>64.206642</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>74.626866</td>\n",
       "      <td>88.495575</td>\n",
       "      <td>80.97166</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>88.79056</td>\n",
       "      <td>91.489362</td>\n",
       "      <td>90.11976</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>94.782609</td>\n",
       "      <td>93.562232</td>\n",
       "      <td>94.168467</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>73.978996</td>\n",
       "      <td>98.830865</td>\n",
       "      <td>84.617951</td>\n",
       "      <td>1268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>100.0</td>\n",
       "      <td>64.705882</td>\n",
       "      <td>78.571429</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>93.922261</td>\n",
       "      <td>93.002099</td>\n",
       "      <td>93.459916</td>\n",
       "      <td>1329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>67.156863</td>\n",
       "      <td>95.804196</td>\n",
       "      <td>78.962536</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>77.5</td>\n",
       "      <td>91.176471</td>\n",
       "      <td>83.783784</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>94.985591</td>\n",
       "      <td>96.430661</td>\n",
       "      <td>95.702671</td>\n",
       "      <td>1648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>85.959745</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.477178</td>\n",
       "      <td>1751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>90.317888</td>\n",
       "      <td>93.624463</td>\n",
       "      <td>91.941455</td>\n",
       "      <td>29179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>82.949309</td>\n",
       "      <td>60.402685</td>\n",
       "      <td>69.902913</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>93.877551</td>\n",
       "      <td>89.61039</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>88.046647</td>\n",
       "      <td>96.178344</td>\n",
       "      <td>91.933029</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>72.642099</td>\n",
       "      <td>89.530408</td>\n",
       "      <td>80.206897</td>\n",
       "      <td>2326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>96.02649</td>\n",
       "      <td>98.639456</td>\n",
       "      <td>97.315436</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>72.60274</td>\n",
       "      <td>54.639175</td>\n",
       "      <td>62.352941</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>47.058824</td>\n",
       "      <td>32.0</td>\n",
       "      <td>38.095238</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>97.297297</td>\n",
       "      <td>47.368421</td>\n",
       "      <td>63.716814</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>93.909348</td>\n",
       "      <td>84.67433</td>\n",
       "      <td>89.053056</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>39.921159</td>\n",
       "      <td>98.532819</td>\n",
       "      <td>56.820965</td>\n",
       "      <td>12760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>90.499391</td>\n",
       "      <td>90.58214</td>\n",
       "      <td>90.540746</td>\n",
       "      <td>2972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>89.853053</td>\n",
       "      <td>90.78459</td>\n",
       "      <td>90.316419</td>\n",
       "      <td>7093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>71.002132</td>\n",
       "      <td>72.972973</td>\n",
       "      <td>71.974063</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>81.108597</td>\n",
       "      <td>77.318476</td>\n",
       "      <td>79.1682</td>\n",
       "      <td>2151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>83.194154</td>\n",
       "      <td>92.35226</td>\n",
       "      <td>87.534322</td>\n",
       "      <td>797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>93.535076</td>\n",
       "      <td>99.125364</td>\n",
       "      <td>96.249115</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>45</td>\n",
       "      <td>89.866667</td>\n",
       "      <td>96.285714</td>\n",
       "      <td>92.965517</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>46</td>\n",
       "      <td>91.640867</td>\n",
       "      <td>92.789969</td>\n",
       "      <td>92.211838</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>accuracy=91.63</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag  precision     recall   f1_score correct_count\n",
       "0             -100          -        0.0          -             0\n",
       "1                0       40.0       25.0  30.769231             4\n",
       "2                1  78.655283  70.123692  74.144869           737\n",
       "3                2          -        0.0          -             0\n",
       "4                3       50.0  22.413793  30.952381            13\n",
       "5                4  88.333333  86.885246  87.603306            53\n",
       "6                5  69.444444  57.803468  63.091483           100\n",
       "7                6  81.533101  98.319328  89.142857           702\n",
       "8                7          -        0.0          -             0\n",
       "9                8  63.225806  76.165803  69.095182           294\n",
       "10              10  93.774704  90.209125  91.957364           949\n",
       "11              11  94.402036  81.359649  87.396938           371\n",
       "12              12  52.095808  83.653846  64.206642            87\n",
       "13              13          -        0.0          -             0\n",
       "14              14  74.626866  88.495575   80.97166           100\n",
       "15              15   88.79056  91.489362   90.11976           301\n",
       "16              16          -        0.0          -             0\n",
       "17              17  94.782609  93.562232  94.168467           218\n",
       "18              18  73.978996  98.830865  84.617951          1268\n",
       "19              19          -        0.0          -             0\n",
       "20              20      100.0  64.705882  78.571429            11\n",
       "21              21  93.922261  93.002099  93.459916          1329\n",
       "22              22  67.156863  95.804196  78.962536           137\n",
       "23              23       77.5  91.176471  83.783784            93\n",
       "24              24  94.985591  96.430661  95.702671          1648\n",
       "25              25  85.959745       85.0  85.477178          1751\n",
       "26              26  90.317888  93.624463  91.941455         29179\n",
       "27              27  82.949309  60.402685  69.902913           360\n",
       "28              28  85.714286  93.877551   89.61039           138\n",
       "29              29  88.046647  96.178344  91.933029           302\n",
       "30              31  72.642099  89.530408  80.206897          2326\n",
       "31              32   96.02649  98.639456  97.315436           145\n",
       "32              33   72.60274  54.639175  62.352941            53\n",
       "33              34  47.058824       32.0  38.095238             8\n",
       "34              35  97.297297  47.368421  63.716814            36\n",
       "35              36  93.909348   84.67433  89.053056           663\n",
       "36              37  39.921159  98.532819  56.820965         12760\n",
       "37              38  90.499391   90.58214  90.540746          2972\n",
       "38              39  89.853053   90.78459  90.316419          7093\n",
       "39              40  71.002132  72.972973  71.974063           999\n",
       "40              41  81.108597  77.318476    79.1682          2151\n",
       "41              42  83.194154   92.35226  87.534322           797\n",
       "42              43  93.535076  99.125364  96.249115           680\n",
       "43              45  89.866667  96.285714  92.965517           337\n",
       "44              46  91.640867  92.789969  92.211838           296\n",
       "45  accuracy=91.63                                               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0BZzVgjm4l_"
   },
   "source": [
    "# Other Pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssZdIst48AwH"
   },
   "source": [
    "In this section, we will experiment by fine-tuning other pretrained models, such as airesearch/wangchanberta-base-wiki-newmm, to see how about their performance.\n",
    "\n",
    "Since each model uses a different word-tokenization method.\n",
    "for example, **airesearch/wangchanberta-base-wiki-newmm uses newmm**,\n",
    "while **airesearch/wangchanberta-base-att-spm-uncased uses SentencePiece**.\n",
    "please try fine-tuning and compare the performance of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUCWuhrl91mj"
   },
   "source": [
    "### #TODO 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:12:38.353745Z",
     "iopub.status.busy": "2025-02-02T14:12:38.353372Z",
     "iopub.status.idle": "2025-02-02T14:12:40.173861Z",
     "shell.execute_reply": "2025-02-02T14:12:40.173170Z",
     "shell.execute_reply.started": "2025-02-02T14:12:38.353717Z"
    },
    "id": "9etT-A_anBfi",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d2635e7f5f4db2a1a7948b0c73ed47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "newmm.json:   0%|          | 0.00/3.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7a1fc5314d4398ae7fef2cdb026b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/559 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'ThaiWordsNewmmTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'ThaiWordsNewmmTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "model_names = [\n",
    "    'airesearch/wangchanberta-base-att-spm-uncased',\n",
    "    'airesearch/wangchanberta-base-wiki-newmm',\n",
    "    'airesearch/wangchanberta-base-wiki-ssg',\n",
    "    'airesearch/wangchanberta-base-wiki-sefr',\n",
    "    'airesearch/wangchanberta-base-wiki-spm',\n",
    "]\n",
    "\n",
    "#@title Choose Pretrained Model\n",
    "model_name = \"airesearch/wangchanberta-base-wiki-newmm\" #@param [\"airesearch/wangchanberta-base-att-spm-uncased\", \"airesearch/wangchanberta-base-wiki-newmm\", \"airesearch/wangchanberta-base-wiki-syllable\", \"airesearch/wangchanberta-base-wiki-sefr\", \"airesearch/wangchanberta-base-wiki-spm\"]\n",
    "\n",
    "#create tokenizer\n",
    "tokenizer = Tokenizer(model_name).from_pretrained(\n",
    "                f'{model_name}',\n",
    "                revision='main',\n",
    "                model_max_length=416,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:12:42.719468Z",
     "iopub.status.busy": "2025-02-02T14:12:42.719160Z",
     "iopub.status.idle": "2025-02-02T14:12:42.735129Z",
     "shell.execute_reply": "2025-02-02T14:12:42.734332Z",
     "shell.execute_reply.started": "2025-02-02T14:12:42.719440Z"
    },
    "id": "LFXdV8V5nGk2",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\n",
      "tokens : ['<s>', 'โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '<unk>', '<_>', 'transfer', '<_>', 'dictionary', ')', '</s>']\n",
      "label tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n"
     ]
    }
   ],
   "source": [
    "example = orchidl[\"train\"][1899]\n",
    "print('sentence :', example[\"sentence\"])\n",
    "tokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print('tokens :',tokens)\n",
    "print('label tokens :', example[\"label_tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7QMwcbK5Ibj"
   },
   "source": [
    "It's the same problem as above.\n",
    "\n",
    "`**Warning: Can we use same function as above ?**`\n",
    "\n",
    "`**Warning: Please beware of <unk>, an unknown word token.**`\n",
    "\n",
    "`**Warning: Please be careful of \" ำ \", the 'am' vowel. WangchanBERTa's internal preprocessing replaces all \" ำ \" to 'ํ' and 'า'**`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:12:45.552876Z",
     "iopub.status.busy": "2025-02-02T14:12:45.552545Z",
     "iopub.status.idle": "2025-02-02T14:12:45.561430Z",
     "shell.execute_reply": "2025-02-02T14:12:45.560528Z",
     "shell.execute_reply.started": "2025-02-02T14:12:45.552852Z"
    },
    "id": "SI0fUulE5MH3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def majority_vote_pos(examples):\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # TO DO: Since the tokens from the output of the pretrained tokenizer\n",
    "    # do not match the tokens in the label tokens of the dataset,\n",
    "    # the task is to create a function to determine the POS tags of the tokens generated by the pretrained tokenizer.\n",
    "    # This should be done by referencing the POS tags in the label tokens. If a token partially overlaps with others,\n",
    "    # the POS tag from the segment with the greater number of characters should be assigned.\n",
    "    #\n",
    "    # Example :\n",
    "    # \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n",
    "    # \"การ\" has a POS tag of 21,\n",
    "    # and \"ประชุม\" has a POS tag of 39.\n",
    "    # Therefore, the POS tag for \"การประชุม\" is 39,\n",
    "    # as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n",
    "    #\n",
    "    # 'ทางวิชาการ' (10 chars) is formed from 'ทาง' (3 chars) + 'วิชาการ' (7 chars)\n",
    "    # \"ทาง\" has a POS tag of 26,\n",
    "    # and \"วิชาการ\" has a POS tag of 2.\n",
    "    # Therefore, the POS tag for \"ทางวิชาการ\" is 2,\n",
    "    # as \"ทางวิชาการ\" is derived more from the \"ทาง\" part than from the \"วิชาการ\" part.\n",
    "\n",
    "    # FILL CODE HERE\n",
    "    # Tokenize the input sentence using the pretrained tokenizer\n",
    "    tokenized_inputs = tokenizer([examples[\"sentence\"]], is_split_into_words=True)\n",
    "    \n",
    "    # Convert the tokenized input IDs to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n",
    "    \n",
    "    # Get the label tokens and POS tags from the example\n",
    "    label_tokens = examples[\"label_tokens\"]\n",
    "    pos_tags = examples[\"pos_tags\"]\n",
    "    original_text = ''.join(label_tokens)\n",
    "    \n",
    "    # Create original spans with POS tags\n",
    "    original_spans = []\n",
    "    current_pos = 0\n",
    "    for token, pos in zip(label_tokens, pos_tags):\n",
    "        end = current_pos + len(token)\n",
    "        original_spans.append((current_pos, end, pos))\n",
    "        current_pos = end\n",
    "    \n",
    "    special_map = {'<s>': '', '</s>': '', '<unk>': ''}\n",
    "    new_pos_result = []\n",
    "    current_orig_pos = 0  # Track position in original text\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Handle special tokens\n",
    "        if token in special_map:\n",
    "            new_pos_result.append(-100)\n",
    "            continue\n",
    "            \n",
    "        # Convert token to original text format\n",
    "        cleaned_token = token.replace('▁', '').replace('ํา', 'ำ')\n",
    "        if token == '<_>':\n",
    "            cleaned_token = ' '\n",
    "        \n",
    "        token_len = len(cleaned_token)\n",
    "        \n",
    "        # Find matching spans\n",
    "        best_pos = -100\n",
    "        max_overlap = 0\n",
    "        \n",
    "        # Check all original spans for overlap\n",
    "        for span_start, span_end, pos in original_spans:\n",
    "            # Calculate potential overlap\n",
    "            overlap_start = max(current_orig_pos, span_start)\n",
    "            overlap_end = min(current_orig_pos + token_len, span_end)\n",
    "            \n",
    "            if overlap_start < overlap_end:\n",
    "                overlap = overlap_end - overlap_start\n",
    "                if overlap > max_overlap:\n",
    "                    max_overlap = overlap\n",
    "                    best_pos = pos\n",
    "        \n",
    "        if best_pos != -100:\n",
    "            new_pos_result.append(best_pos)\n",
    "            # Advance by full token length (spaces are part of multi-word tokens)\n",
    "            current_orig_pos += token_len\n",
    "        else:\n",
    "            new_pos_result.append(-100)\n",
    "            current_orig_pos += token_len\n",
    "\n",
    "    tokenized_inputs['tokens'] = tokens\n",
    "    tokenized_inputs['labels'] = new_pos_result\n",
    "    return tokenized_inputs\n",
    "    ####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:12:49.067231Z",
     "iopub.status.busy": "2025-02-02T14:12:49.066952Z",
     "iopub.status.idle": "2025-02-02T14:13:08.676615Z",
     "shell.execute_reply": "2025-02-02T14:13:08.675764Z",
     "shell.execute_reply.started": "2025-02-02T14:12:49.067209Z"
    },
    "id": "O5n4veYxo3rR",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function majority_vote_pos at 0x7d22118a01f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0708acb128ed44f9af4edf737271c8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f3e82ebbb049b9a32b977fb3e321c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4625 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_orchid = orchidl.map(majority_vote_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:13:08.678060Z",
     "iopub.status.busy": "2025-02-02T14:13:08.677818Z",
     "iopub.status.idle": "2025-02-02T14:13:08.691376Z",
     "shell.execute_reply": "2025-02-02T14:13:08.690575Z",
     "shell.execute_reply.started": "2025-02-02T14:13:08.678038Z"
    },
    "id": "ashRh72szWiM",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 1899\n",
      "label_tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n",
      "pos_tags : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n",
      "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\n",
      "input_ids : [0, 80, 3973, 45, 12252, 3496, 592, 5, 3, 5, 30055, 5, 63190, 178, 2]\n",
      "token_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "tokens : ['<s>', 'โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '<unk>', '<_>', 'transfer', '<_>', 'dictionary', ')', '</s>']\n",
      "labels : [-100, 25, 39, 38, 26, 26, 5, 37, -100, 37, 26, 26, 26, 26, -100]\n"
     ]
    }
   ],
   "source": [
    "# hard test case\n",
    "example = tokenized_orchid[\"train\"][1899]\n",
    "for i in example :\n",
    "    print(i, \":\", example[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:13:08.693162Z",
     "iopub.status.busy": "2025-02-02T14:13:08.692885Z",
     "iopub.status.idle": "2025-02-02T14:13:13.917352Z",
     "shell.execute_reply": "2025-02-02T14:13:13.916706Z",
     "shell.execute_reply.started": "2025-02-02T14:13:08.693141Z"
    },
    "id": "7AL0Vqbv7cfb",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1ce16771f249e3b4f617d628822c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/646M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-wiki-newmm were not used when initializing RobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-wiki-newmm and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_names = [\n",
    "    'wangchanberta-base-att-spm-uncased',\n",
    "    'wangchanberta-base-wiki-newmm',\n",
    "    'wangchanberta-base-wiki-ssg',\n",
    "    'wangchanberta-base-wiki-sefr',\n",
    "    'wangchanberta-base-wiki-spm',\n",
    "]\n",
    "\n",
    "#@title Choose Pretrained Model\n",
    "model_name = \"wangchanberta-base-wiki-newmm\" #@param [\"wangchanberta-base-att-spm-uncased\", \"wangchanberta-base-wiki-newmm\", \"wangchanberta-base-wiki-syllable\", \"wangchanberta-base-wiki-sefr\", \"wangchanberta-base-wiki-spm\"]\n",
    "\n",
    "#create model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    f\"airesearch/{model_name}\",\n",
    "    revision='main',\n",
    "    num_labels=47, id2label=id2label, label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:13:13.918437Z",
     "iopub.status.busy": "2025-02-02T14:13:13.918207Z",
     "iopub.status.idle": "2025-02-02T14:13:13.922610Z",
     "shell.execute_reply": "2025-02-02T14:13:13.921911Z",
     "shell.execute_reply.started": "2025-02-02T14:13:13.918417Z"
    },
    "id": "pWTh0bMfP70g",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkhYDS4q7oxK"
   },
   "source": [
    "### #TODO 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVdITM5E7tQ5"
   },
   "source": [
    "Fine-tuning other pretrained model with our orchid corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:13:17.912071Z",
     "iopub.status.busy": "2025-02-02T14:13:17.911750Z",
     "iopub.status.idle": "2025-02-02T14:23:27.194767Z",
     "shell.execute_reply": "2025-02-02T14:23:27.194061Z",
     "shell.execute_reply.started": "2025-02-02T14:13:17.912043Z"
    },
    "id": "hBHlUamr7syk",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1737' max='1737' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1737/1737 10:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.669300</td>\n",
       "      <td>0.559264</td>\n",
       "      <td>0.780582</td>\n",
       "      <td>0.746733</td>\n",
       "      <td>0.763283</td>\n",
       "      <td>0.825169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>0.523616</td>\n",
       "      <td>0.762283</td>\n",
       "      <td>0.767600</td>\n",
       "      <td>0.764932</td>\n",
       "      <td>0.832611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.489600</td>\n",
       "      <td>0.513638</td>\n",
       "      <td>0.767856</td>\n",
       "      <td>0.765481</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.834403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2254: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1737, training_loss=0.6497774545530101, metrics={'train_runtime': 607.7074, 'train_samples_per_second': 91.327, 'train_steps_per_second': 2.858, 'total_flos': 1371637012822776.0, 'train_loss': 0.6497774545530101, 'epoch': 3.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    #########################\n",
    "    output_dir=\"./results2\",  # Directory where the model checkpoints and logs will be saved\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    learning_rate=2e-5,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=16,  # Batch size per GPU/CPU for training\n",
    "    per_device_eval_batch_size=16,  # Batch size per GPU/CPU for evaluation\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Strength of weight decay\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    save_total_limit=2,  # Limit the number of saved checkpoints\n",
    "    push_to_hub=False,  # Set to True if you want to upload the model to the Hugging Face Hub\n",
    "    logging_dir=\"./logs\",  # Directory for storing logs\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to determine the best model\n",
    "    greater_is_better=False,  # Lower eval_loss is better\n",
    "    #########################\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    #########################\n",
    "    model=model,  # The pre-trained model\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=tokenized_orchid['train'],  # Training dataset\n",
    "    eval_dataset=tokenized_orchid['test'],  # Evaluation dataset\n",
    "    tokenizer=tokenizer,  # Tokenizer\n",
    "    data_collator=data_collator,  # Data collator\n",
    "    compute_metrics=compute_metrics,  # Function to compute metrics\n",
    "    ########################\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:26:06.970363Z",
     "iopub.status.busy": "2025-02-02T14:26:06.970068Z",
     "iopub.status.idle": "2025-02-02T14:26:57.129343Z",
     "shell.execute_reply": "2025-02-02T14:26:57.128532Z",
     "shell.execute_reply.started": "2025-02-02T14:26:06.970340Z"
    },
    "id": "KIdxvgUGCVsm",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460837c876d243c081e80609ba47a940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 29, 37, 21, 39, 26, 26, 37, 26, 26, 41, 37, 26, 39, 39, 26, 37]\n",
      "[-100, 29, 37, 21, 39, 26, 26, 37, 26, 26, 41, 37, 26, 39, 39, 26, -100]\n",
      "speacial_tag : 11485\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>25.806452</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>54.995656</td>\n",
       "      <td>59.829868</td>\n",
       "      <td>57.311</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>3.252033</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>24.561404</td>\n",
       "      <td>34.567901</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>92.0</td>\n",
       "      <td>71.875</td>\n",
       "      <td>80.701754</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>71.311475</td>\n",
       "      <td>54.375</td>\n",
       "      <td>61.702128</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>75.623269</td>\n",
       "      <td>60.532151</td>\n",
       "      <td>67.241379</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>65.724382</td>\n",
       "      <td>57.407407</td>\n",
       "      <td>61.285008</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>68.264111</td>\n",
       "      <td>76.583035</td>\n",
       "      <td>72.184685</td>\n",
       "      <td>641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>92.256637</td>\n",
       "      <td>82.738095</td>\n",
       "      <td>87.238494</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>54.074074</td>\n",
       "      <td>68.867925</td>\n",
       "      <td>60.580913</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>84.920635</td>\n",
       "      <td>74.825175</td>\n",
       "      <td>79.553903</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>91.891892</td>\n",
       "      <td>85.804416</td>\n",
       "      <td>88.743883</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>87.279152</td>\n",
       "      <td>90.808824</td>\n",
       "      <td>89.009009</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>82.516892</td>\n",
       "      <td>74.69419</td>\n",
       "      <td>78.410915</td>\n",
       "      <td>977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>79.448723</td>\n",
       "      <td>85.291558</td>\n",
       "      <td>82.266527</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>80.851064</td>\n",
       "      <td>73.076923</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>86.206897</td>\n",
       "      <td>72.815534</td>\n",
       "      <td>78.947368</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>90.804598</td>\n",
       "      <td>87.468546</td>\n",
       "      <td>89.105358</td>\n",
       "      <td>1738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>86.167899</td>\n",
       "      <td>82.335827</td>\n",
       "      <td>84.208289</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>68.877406</td>\n",
       "      <td>87.689531</td>\n",
       "      <td>77.153294</td>\n",
       "      <td>21861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>57.006369</td>\n",
       "      <td>38.166311</td>\n",
       "      <td>45.721584</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>88.727273</td>\n",
       "      <td>91.044776</td>\n",
       "      <td>89.871087</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>73.44782</td>\n",
       "      <td>88.394277</td>\n",
       "      <td>80.23088</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>56.111508</td>\n",
       "      <td>76.697606</td>\n",
       "      <td>64.809082</td>\n",
       "      <td>1570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>58.823529</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>66.985646</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>55.05618</td>\n",
       "      <td>59.036145</td>\n",
       "      <td>56.976744</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>81.818182</td>\n",
       "      <td>78.26087</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>67.479675</td>\n",
       "      <td>67.479675</td>\n",
       "      <td>67.479675</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>92.416667</td>\n",
       "      <td>81.484203</td>\n",
       "      <td>86.606794</td>\n",
       "      <td>1109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>61.374538</td>\n",
       "      <td>85.235339</td>\n",
       "      <td>71.363262</td>\n",
       "      <td>7805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>90.47309</td>\n",
       "      <td>83.933964</td>\n",
       "      <td>87.08094</td>\n",
       "      <td>4169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>74.493884</td>\n",
       "      <td>86.464325</td>\n",
       "      <td>80.033985</td>\n",
       "      <td>7065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>65.661642</td>\n",
       "      <td>58.90308</td>\n",
       "      <td>62.09901</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>76.427256</td>\n",
       "      <td>74.819712</td>\n",
       "      <td>75.614941</td>\n",
       "      <td>2490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>85.55102</td>\n",
       "      <td>85.064935</td>\n",
       "      <td>85.307285</td>\n",
       "      <td>1048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>91.187271</td>\n",
       "      <td>92.20297</td>\n",
       "      <td>91.692308</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>45</td>\n",
       "      <td>88.378378</td>\n",
       "      <td>95.614035</td>\n",
       "      <td>91.853933</td>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>46</td>\n",
       "      <td>88.300836</td>\n",
       "      <td>88.547486</td>\n",
       "      <td>88.423989</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>accuracy=83.44</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag  precision     recall   f1_score correct_count\n",
       "0             -100          -        0.0          -             0\n",
       "1                0       25.0  26.666667  25.806452             4\n",
       "2                1  54.995656  59.829868     57.311           633\n",
       "3                2  18.181818   1.785714   3.252033             2\n",
       "4                3  58.333333  24.561404  34.567901            14\n",
       "5                4       92.0     71.875  80.701754            46\n",
       "6                5  71.311475     54.375  61.702128            87\n",
       "7                6  75.623269  60.532151  67.241379           273\n",
       "8                7          -        0.0          -             0\n",
       "9                8  65.724382  57.407407  61.285008           186\n",
       "10              10  68.264111  76.583035  72.184685           641\n",
       "11              11  92.256637  82.738095  87.238494           417\n",
       "12              12  54.074074  68.867925  60.580913            73\n",
       "13              13          -        0.0          -             0\n",
       "14              14  84.920635  74.825175  79.553903           107\n",
       "15              15  91.891892  85.804416  88.743883           272\n",
       "16              16          -        0.0          -             0\n",
       "17              17  87.279152  90.808824  89.009009           247\n",
       "18              18  82.516892   74.69419  78.410915           977\n",
       "19              19          -        0.0          -             0\n",
       "20              20      100.0       60.0       75.0             9\n",
       "21              21  79.448723  85.291558  82.266527          1960\n",
       "22              22  66.666667  80.851064  73.076923           114\n",
       "23              23  86.206897  72.815534  78.947368            75\n",
       "24              24  90.804598  87.468546  89.105358          1738\n",
       "25              25  86.167899  82.335827  84.208289          1981\n",
       "26              26  68.877406  87.689531  77.153294         21861\n",
       "27              27  57.006369  38.166311  45.721584           179\n",
       "28              28  88.727273  91.044776  89.871087           244\n",
       "29              29   73.44782  88.394277   80.23088           556\n",
       "30              31  56.111508  76.697606  64.809082          1570\n",
       "31              32  58.823529  77.777778  66.985646            70\n",
       "32              33   55.05618  59.036145  56.976744            49\n",
       "33              34       75.0  81.818182   78.26087            18\n",
       "34              35  67.479675  67.479675  67.479675            83\n",
       "35              36  92.416667  81.484203  86.606794          1109\n",
       "36              37  61.374538  85.235339  71.363262          7805\n",
       "37              38   90.47309  83.933964   87.08094          4169\n",
       "38              39  74.493884  86.464325  80.033985          7065\n",
       "39              40  65.661642   58.90308   62.09901           784\n",
       "40              41  76.427256  74.819712  75.614941          2490\n",
       "41              42   85.55102  85.064935  85.307285          1048\n",
       "42              43  91.187271   92.20297  91.692308           745\n",
       "43              45  88.378378  95.614035  91.853933           981\n",
       "44              46  88.300836  88.547486  88.423989           317\n",
       "45  accuracy=83.44                                               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######## EVALUATE YOUR MODEL ########\n",
    "test_data = tokenized_orchid[\"test\"]\n",
    "y_test = [inputs['labels'] for inputs in test_data]\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"results2/checkpoint-1737\") \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "y_pred = []\n",
    "for _, inputs in enumerate(tqdm(test_data)):\n",
    "    text = inputs['sentence']\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(**inputs).logits\n",
    "        predictions = torch.argmax(pred, dim=2)\n",
    "        # Append padded predictions to y_pred\n",
    "        y_pred.append(predictions.tolist()[0])\n",
    "\n",
    "print(y_pred[0])\n",
    "print(y_test[0])\n",
    "\n",
    "evaluation_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMpXErqPBv2I"
   },
   "source": [
    "### #TODO 5\n",
    "\n",
    "Compare the results between both models. Are they comparable? (Think about the ground truths of both models).\n",
    "\n",
    "Propose a way to fairly evaluate the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UigMzZVNCDuS"
   },
   "source": [
    "<b>Write your answer here :</b>   \n",
    "#### att-spm-uncased:  \n",
    "- higher accuracy (91.63% vs. 83.44%)  \n",
    "- higher F1 scores for most tags (91.94 vs. 77.15 for tag 26).  \n",
    "- means this model is better at common words  \n",
    "#### newmm:  \n",
    "- Lower accuracy but better performance on some tags (tag 37 F1: 71.36 vs. 56.82)  \n",
    "- means this model is better at punctuation <_>  \n",
    "\n",
    "#### Not a fair comparison, because of ground truth inconsistencies (has different pos tag)  \n",
    "#### To ensure a fair comparision:  \n",
    "- use shared dataset: to have same pos tag in both model  \n",
    "- use macro f1: to reduce bias from class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYN0X9HWVuUe"
   },
   "source": [
    "A note on preprocessing data.\n",
    "\n",
    "``process_transformers`` in ``thaixtransformers.preprocess`` also provides a preprocess code that deals with many issues such as casing, text cleaning, and white space replacement with <_>. You can also use this to preprocess your text. Note that space replacement is done automatically without preprocessing in thaixtransformers.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
