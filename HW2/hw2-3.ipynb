{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iU5fRQwhEdJy"
   },
   "source": [
    "# Subword Tokenization\n",
    "\n",
    "In this exercise, we will learn how to train our own subword tokenizers with different algorithms: BPE and Unigram. We will use `sentencepiece`, a library from Google to help create our tokenizers.\n",
    "\n",
    "## Ref:\n",
    "https://github.com/google/sentencepiece/blob/master/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI9gRZlUE80g"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:15:28.028561Z",
     "iopub.status.busy": "2025-01-18T17:15:28.028230Z",
     "iopub.status.idle": "2025-01-18T17:15:29.703145Z",
     "shell.execute_reply": "2025-01-18T17:15:29.702031Z",
     "shell.execute_reply.started": "2025-01-18T17:15:28.028537Z"
    },
    "id": "1pOsV-jaW975",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-18 17:15:28--  https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt [following]\n",
      "--2025-01-18 17:15:28--  https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3231076 (3.1M) [application/octet-stream]\n",
      "Saving to: ‘pra-apai-manee-ch1-50.txt.2’\n",
      "\n",
      "pra-apai-manee-ch1- 100%[===================>]   3.08M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2025-01-18 17:15:28 (46.2 MB/s) - ‘pra-apai-manee-ch1-50.txt.2’ saved [3231076/3231076]\n",
      "\n",
      "--2025-01-18 17:15:28--  https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/kratoo-40000000-40002000.jsonl\n",
      "Resolving github.com (github.com)... 140.82.112.4\n",
      "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/kratoo-40000000-40002000.jsonl [following]\n",
      "--2025-01-18 17:15:29--  https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/kratoo-40000000-40002000.jsonl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2968483 (2.8M) [text/plain]\n",
      "Saving to: ‘kratoo-40000000-40002000.jsonl.2’\n",
      "\n",
      "kratoo-40000000-400 100%[===================>]   2.83M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2025-01-18 17:15:29 (38.8 MB/s) - ‘kratoo-40000000-40002000.jsonl.2’ saved [2968483/2968483]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
    "!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/kratoo-40000000-40002000.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSiDpG9WE-cT"
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:15:29.704638Z",
     "iopub.status.busy": "2025-01-18T17:15:29.704400Z",
     "iopub.status.idle": "2025-01-18T17:15:29.708109Z",
     "shell.execute_reply": "2025-01-18T17:15:29.707348Z",
     "shell.execute_reply.started": "2025-01-18T17:15:29.704617Z"
    },
    "id": "OQd7M6gLWPLN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OifbmMIstzs8"
   },
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:15:29.710014Z",
     "iopub.status.busy": "2025-01-18T17:15:29.709769Z",
     "iopub.status.idle": "2025-01-18T17:15:29.745980Z",
     "shell.execute_reply": "2025-01-18T17:15:29.745400Z",
     "shell.execute_reply.started": "2025-01-18T17:15:29.709995Z"
    },
    "id": "-FnIDvb1lMuh",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060318"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pantip_text = []\n",
    "with open('kratoo-40000000-40002000.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "    for json_str in json_list:\n",
    "        result = json.loads(json_str)\n",
    "        pantip_text.append(f\"{result['title']}\\n{result['content']}\\n\")\n",
    "sum([len(t) for t in pantip_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:15:29.747303Z",
     "iopub.status.busy": "2025-01-18T17:15:29.747076Z",
     "iopub.status.idle": "2025-01-18T17:15:29.767470Z",
     "shell.execute_reply": "2025-01-18T17:15:29.766931Z",
     "shell.execute_reply.started": "2025-01-18T17:15:29.747278Z"
    },
    "id": "yaaQVXZ8A0j1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"pra-apai-manee-ch1-50.txt\") as f:\n",
    "  pra_apai_manee_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:15:29.768519Z",
     "iopub.status.busy": "2025-01-18T17:15:29.768241Z",
     "iopub.status.idle": "2025-01-18T17:15:29.775015Z",
     "shell.execute_reply": "2025-01-18T17:15:29.774118Z",
     "shell.execute_reply.started": "2025-01-18T17:15:29.768491Z"
    },
    "id": "LksJKc9MA5F_",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100605"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(t) for t in pra_apai_manee_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:15:29.776171Z",
     "iopub.status.busy": "2025-01-18T17:15:29.775898Z",
     "iopub.status.idle": "2025-01-18T17:15:29.788267Z",
     "shell.execute_reply": "2025-01-18T17:15:29.787450Z",
     "shell.execute_reply.started": "2025-01-18T17:15:29.776141Z"
    },
    "id": "RbdfkF-vAoie",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pantip_train_text = pantip_text[:int(len(pantip_text)*0.8)]\n",
    "pantip_test_text = pantip_text[int(len(pantip_text)*0.8):]\n",
    "\n",
    "pam_train_text = pra_apai_manee_data[:int(len(pra_apai_manee_data)*0.8)] #pam = pra_apai_manee\n",
    "pam_test_text = pra_apai_manee_data[int(len(pra_apai_manee_data)*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhwcH0Aot1XI"
   },
   "source": [
    "## Run tokenizer training\n",
    "\n",
    "The Python wrapper provides multiple APIs for training our tokenizers\n",
    "\n",
    "1. `spm.SentencePieceTrainer.train(input='input.txt', model_prefix='m', vocab_size=vocab_size, model_type=model_type)`\n",
    "  <br> This will output the tokenizer files `m.model` and `m.vocab` that can be later loaded into `SentencePieceProcessor`.\n",
    "  <br><br>\n",
    "2. `spm.SentencePieceTrainer.train(sentence_iterator=iterator, model_writer=obj_with_write_method, vocab_size=vocab_size, model_type=model_type)`\n",
    "  <br> This method will require a file object e.g. `obj_with_write_method = io.BytesIO()`. The advantage of this method is you can run sentencepiece on environments that have limited access to the local file system. But you will still have to save the model file if you want to re-use the model else you will have to train it again.\n",
    "<br><br>\n",
    "3.  `spm.SentencePieceTrainer.train('--input=input.txt --model_prefix=m --vocab_size=vocab_size --model_type=model_type')`\n",
    "<br> Same as no.1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:15:29.789233Z",
     "iopub.status.busy": "2025-01-18T17:15:29.788927Z",
     "iopub.status.idle": "2025-01-18T17:15:29.820010Z",
     "shell.execute_reply": "2025-01-18T17:15:29.819383Z",
     "shell.execute_reply.started": "2025-01-18T17:15:29.789214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"pantip-train.txt\", \"w\") as f:\n",
    "    f.write(\"\".join(pantip_train_text))\n",
    "with open(\"pantip-test.txt\", \"w\") as f:\n",
    "    f.write(\"\".join(pantip_test_text))\n",
    "with open(\"pam-train.txt\", \"w\") as f:\n",
    "    f.write(\"\".join(pam_train_text))\n",
    "with open(\"pam-test.txt\", \"w\") as f:\n",
    "    f.write(\"\".join(pam_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3XeFFYw-T_0"
   },
   "source": [
    "### Unigram tokenizer\n",
    "\n",
    "We are going to start with training a unigram tokenizer. You can use any method of training one. Make sure to set vocab_size to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:15:29.822100Z",
     "iopub.status.busy": "2025-01-18T17:15:29.821893Z",
     "iopub.status.idle": "2025-01-18T17:15:40.735749Z",
     "shell.execute_reply": "2025-01-18T17:15:40.734556Z",
     "shell.execute_reply.started": "2025-01-18T17:15:29.822082Z"
    },
    "id": "bFCfHphd15g9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Train\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='pam-train.txt',\n",
    "    model_prefix='unigram-pam',\n",
    "    vocab_size=1000,\n",
    "    model_type='unigram'\n",
    ")\n",
    "\n",
    "sp_pam_unigram = spm.SentencePieceProcessor(model_file='unigram-pam.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdXPaoW3_v2T"
   },
   "source": [
    "### Q1 MCV\n",
    "\n",
    "How many tokens did you get when tokenizing the following sentence with your unigram tokenizer: <br>\n",
    "'อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:15:40.738103Z",
     "iopub.status.busy": "2025-01-18T17:15:40.737755Z",
     "iopub.status.idle": "2025-01-18T17:15:40.751649Z",
     "shell.execute_reply": "2025-01-18T17:15:40.750819Z",
     "shell.execute_reply.started": "2025-01-18T17:15:40.738075Z"
    },
    "id": "J1bO3s-z-PLb",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_pam = spm.SentencePieceProcessor(model_file='unigram-pam.model')\n",
    "len(sp_pam.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKkc1D-hAFxl"
   },
   "source": [
    "### BPE Tokenizer\n",
    "\n",
    "Now try training a BPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:15:40.752901Z",
     "iopub.status.busy": "2025-01-18T17:15:40.752589Z",
     "iopub.status.idle": "2025-01-18T17:15:42.962464Z",
     "shell.execute_reply": "2025-01-18T17:15:42.961675Z",
     "shell.execute_reply.started": "2025-01-18T17:15:40.752851Z"
    },
    "id": "AiXj57rh-PIv",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input='pam-train.txt',\n",
    "    model_prefix='bpe-pam',\n",
    "    vocab_size=1000,\n",
    "    model_type='bpe'\n",
    ")\n",
    "\n",
    "sp_pam_bpe = spm.SentencePieceProcessor(model_file='bpe-pam.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrQwGmL5AMXc"
   },
   "source": [
    "### Q2 MCV\n",
    "\n",
    "How many tokens did you get when tokenizing the following sentence with your BPE tokenizer: <br>\n",
    "'อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:16:05.860835Z",
     "iopub.status.busy": "2025-01-18T17:16:05.860522Z",
     "iopub.status.idle": "2025-01-18T17:16:05.867205Z",
     "shell.execute_reply": "2025-01-18T17:16:05.866431Z",
     "shell.execute_reply.started": "2025-01-18T17:16:05.860808Z"
    },
    "id": "0AXuzyaN-PEr",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe = spm.SentencePieceProcessor(model_file='bpe-pam.model')\n",
    "len(bpe.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbb6C6-IS_Ly"
   },
   "source": [
    "These are some of your vocabs. Note that you will see \"▁\" (U+2581) in every type of tokenizer in SentencePiece since it makes it possible to perform detokenization \\(unsplit your sentences\\) without relying on language-specific resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:16:08.464067Z",
     "iopub.status.busy": "2025-01-18T17:16:08.463737Z",
     "iopub.status.idle": "2025-01-18T17:16:08.470991Z",
     "shell.execute_reply": "2025-01-18T17:16:08.470174Z",
     "shell.execute_reply.started": "2025-01-18T17:16:08.464038Z"
    },
    "id": "Aa9j6XrTKjyA",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> | <s> | </s> | ▁ | า | เ | น | ม | ย | ก | ร | ว | ด | ส | ง | บ | ค | มา | อ | ล | จะ | ท | ให้ | ห | ไป | ไม่ | แ | ว่า | พ | ุ | ี | ๏ | ฯ | ข | ช | เป็น | พระ | โ | ที่ | ใจ | ▁จะ | จ | ะ | ิ | ต | ก็ | อยู่ | ป | ได้ | ่ | ไ | เข้า | ู | ▁พระ | ้า | ตาม | ใน | ้ | ▁แล้ว | เหมือน | รา | ศ | เจ้า | เห็น | ลา | กัน | ั | หา | นาง | ทรง | ประ | ์ | ยา | ัก | ํา | ซ | าน | ัง | ฉ | องค์ | ัด | แล้ว | อน | ดู | ถ | ด้วย | มี | ▁จึง | นี้ | ่า | ผ | น้อง | แต่ | ทํา | ▁นาง | ▁ให้ | รัก | พี่ | คิด | ลูก | พา | รู้ | การ | กับ | ัน | หน้า | กระ | วน | ออก | ่อ | เขา | ถึง | ระ | ข้า | ับ | พล | นั่ง | ทั้ง | หน | รับ | ษ | กล | วง | ลง | ฝ | กร | พร | ความ | เสีย | ดี | ขึ้น | อง | ่ง | ธ | ▁แต่ | คน | กลับ | ▁ฝ่าย | ้น | อด | ภ | หรือ | ตร | ือ | ฟัง | แม่ | ▁ไม่ | ไว้ | ยัง | ▁เห็น | นา | ขอ | มิ | น้ํา | หล | ดัง | ▁พอ | ▁ทั้ง | ช่วย | สม | นั้น | ริ | ทัพ | ต้อง | วัน | อา | น้อย | รบ | ิน | อย่า | เอา | จน | เรา | สุด | เสียง | ข้าง | หลัง | ตี | ตัว | ละ | สุ | วัง | ทุก | ่น | ึก | นึก | เฝ้า | นาย | ฝรั่ง | ทูล | เส | วิ | ปล | ▁ถึง | ตาย | ใคร | อก | อั | ตา | เรือ | จึง | แล | ี่ | ั่ง | แสน | สอง | ของ | ็ | ลี | ี้ | จิต | หมาย | ้ม | แจ้ง | ั่น | สั่ง | ราช | พิ | เห | หาย | ้อง | เมือง | เหลือ | กลาง | กษัตริย์ | ยิ่ง | ตรัส | ึง | เลย | เล่า | ทาง | ุด | ศรี | เคย | ไหน | สาม | หนี | ณ | มัน | ื้อ | ค่อย | ชาย | พราหมณ์ | ▁อย่า | ญ | ที | นิ | น่า | สิ้น | ฉัน | กาย | ลังกา | ▁ด้วย | คอย | บอก | สิ | ฟ | สงสาร | พ่อ | ยง | จริง | ชาว | ถาม | ไร | ทหาร | ตั้ง | ▁อัน | เที่ยว | ปร | ผู้ | พวก | สาร | ชม | ศึก | คํา | ▁เป็น | ทอง | อบ | ใหญ่ | ถือ | สาว | พระอภัย | จง | สา | จับ | ั้น | พลาง | ▁มา | ยก | ▁บ้าง | ไพร่ | ลม | ล้วน | ▁ต่าง | ร้อย | พบ | งาม | แกล้ง | อาย | จะได้ | เคียง | อย่าง | เครื่อง | กลัว | ลาย | จํา | ต่าง | สินสมุทร | ▁พวก | ม้า | ลํา | นี่ | ผา | แก้ว | เพราะ | ▁ครั้น | ▁จน | ▁แม้น | สาย | พัน | พระองค์ | พร้อม | วาย | ชิง | ห้อง | ร้อง | สู้ | ▁จง | ลิ | ราย | ล่อ | จาก | ้ว | ท่าน | รอง | เดิน | เรียก | ขัด | เหล่า | กุมาร | ผล | ป่า | ู่ | คู่ | รูป | กิน | พอ | ร่ํา | โฉม | ▁ถ้า | คง | ่าย | ใช้ | ตอบ | หลง | ไล่ | จัด | ดับ | ▁เมื่อ | บน | อ่อน | แสง | คืน | ใส่ | แค้น | รถ | ตรง | แต่ง | แน่ | เชิญ | ชื่น | ถวาย | โห | จร | มิได้ | นอน | ุก | ชวน | เมีย | อาลัย | ้อม | ลับ | ไหว | ▁แม้ | บิดา | หญิง | หลับ | ดอก | กล้า | ขาด | จัก | ไม่มี | บาท | เสนา | ย์ | ช่าง | โศก | วาง | ติด | เสร็จ | ร้อน | คุณ | ผัว | นัก | ความตาม | พักตร์ | หน่อ | ้อย | ▁ซึ่ง | ตะ | ห้าม | พราย | ฟ้า | ไฉน | ใ | ตก | เมื่อ | ยศ | ชล | ดํา | หนึ่ง | ผัน | ใด | สัก | ร้าย | วิ่ง | แก้ | ยาม | ศรีสุวรรณ | ปืน | ฆ่า | ขับ | ขวา | ไฟ | พูด | หมอง | ก็ไม่ | กําลัง | รักษา | เช่น | ุ่ม | ผี | หาญ | เล่น | เนื้อ | รีบ | ถูก | ชัย | บุตรี | ฟัน | บ้าง | เอ๋ย | สงสัย | ผิด | นิ่ง | ชื่อ | เถิด | ผ่อน | หลาน | สี่ | ชาติ | อี | ปาก | ช้า | ึ | แตก | ตรา | รณ | ลอง | ปี | หมอ | เจ้าพราหมณ์ | พี่เลี้ยง | ต่อ | พลอย | โฉมยง | เนตร | หัก | กอด | เชย | ทั้งสอง | ยิ้ม | ค่ํา | นอก | ขวัญ | ซ้ํา | อารมณ์ | ทุกข์ | แขก | เย็น | หนักหนา | ั้ง | ปิด | โปรด | ้ง | กําปั่น | เรียง | แรง | สิ่ง | เศร้า'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_vocabs = [sp_pam.id_to_piece(id) for id in range(sp_pam.get_piece_size())]\n",
    "\" | \".join(unigram_vocabs[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:16:10.742685Z",
     "iopub.status.busy": "2025-01-18T17:16:10.742252Z",
     "iopub.status.idle": "2025-01-18T17:16:10.749436Z",
     "shell.execute_reply": "2025-01-18T17:16:10.748679Z",
     "shell.execute_reply.started": "2025-01-18T17:16:10.742654Z"
    },
    "id": "2TsXA0UqN5LN",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> | <s> | </s> | ้า | ่า | อง | ระ | ํา | รา | อย | ่ง | มา | จะ | ัง | ัน | ▁เ | าย | ้ว | ับ | ี่ | ม่ | อน | ให | าม | ้น | ็น | พระ | ีย | าง | กล | ้ง | ัก | หน | ให้ | ไม่ | หล | ่น | ึง | ▁แ | ทั | ตร | าร | ้อง | ไป | ิด | ข้า | ว่า | หม | คร | ือ | ล้ว | เป | เส | ประ | าน | ั่ง | ▁๏ | ▁ฯ | ที่ | อก | เล | ิน | ได | พล | ทร | ัด | นาง | ึก | ได้ | ู่ | ▁จะ | ค์ | ี้ | พร | เป็น | สุ | ทั้ง | อม | ัย | เร | ห็น | ▁จ | ▁พระ | ก็ | ใจ | อา | ื่ | ่าง | ต่ | กร | ิง | วง | วน | ือน | เจ | ู้ | ียง | อยู่ | รร | ตาม | ▁พ | ้วย | าว | ถึง | คล | ั้น | รี | เข | ด้วย | สม | องค์ | สน | าก | ▁แล้ว | เช | ัว | ย์ | ใน | คว | น้ | หมือน | ▁ส | ูก | อบ | กระ | เจ้า | ทรง | ลา | กัน | มี | ่าย | พรา | ิ่ง | เข้า | เห็น | ิต | สง | อด | ณ์ | วย | ้ม | คิด | เม | เก | เด | ▁นาง | วา | ุก | ▁ให้ | ดู | หา | ▁อ | ▁จึง | ทํา | ลง | รัก | เค | แล้ว | ่าน | พี่ | เหมือน | ั่น | ความ | ยง | อย่า | หร | มิ | ืน | ช่ | การ | ัญ | ▁ไม่ | ฝ่าย | ศรี | ้าง | วก | ้อม | ือง | น้อง | ยว | พา | แก | กํา | ่อน | ื่น | หน้า | ยา | ดี | ั้ง | ▁ทั้ง | ปรา | คน | เน | หว | รับ | แต่ | ้าย | ัส | เหล | ดา | สํา | นี้ | สาร | กับ | ลูก | ละ | ▁ต | รู้ | ื่อ | ▁ฝ่าย | ึ่ง | ลัง | าด | ื้ | กา | ขึ | นั่ง | เท | ▁เห็น | ฟัง | ้อย | ไร | ขึ้น | เสีย | ▁แต่ | บุ | สา | ไว | ทุก | กลับ | สุด | ัต | ใคร | น้ํา | ชา | ุด | ทัพ | วัน | สอง | นา | หย | ตา | รบ | ▁มา | ่อ | หรือ | ทู | ยัง | รง | จร | ปร | ▁บ | ไว้ | ดัง | วิ | ช่วย | ปล | ออก | ัตร | เพ | สิ | แจ | แล | ็จ | ิย์ | ▁พอ | มาร | ค่ | วรร | หมณ์ | คํา | เขา | นั้น | กษ | เย | ข้าง | หมา | เว | ไพร | หลัง | จิต | พราหมณ์ | ้ํา | ▁ถึง | ขอ | ทูล | สาม | ื้อ | วาย | อภ | ทาง | ▁แม | วัง | โฉ | ่ม | จน | ▁เป | ัตริย์ | ื่อง | สั่ง | แม่ | ▁ช | ฝ้า | โฉม | ราช | ฝร | ▁ถ | ฝรั่ง | ิ์ | ลม | แต | ▁เป็น | หาร | ื้น | เห | ้อน | ตาย | ุ่ง | ตัว | อย่าง | ลี | ผู้ | น้อย | ฉัน | ตรี | กุ | ษา | ุทร | ถาม | ของ | พร้อม | ชี | สร | เอ | ุง | พลาง | ตี | สมุทร | หาย | ที | วรรณ | เลี้ | นึก | จึง | หมาย | ▁ด้วย | ขว | ียน | ศึก | ่อง | ต้อง | ลัย | บา | พิ | อุ | สุวรรณ | โย | เรา | กลาง | เฝ้า | กษัตริย์ | สะ | แท | สัย | แจ้ง | หญ | ▁อย่า | รํา | ตรัส | อภัย | ผล | เลย | ียว | ไหน | ้าว | แน | ิดา | ริ | สาว | ิ้ม | เมือง | เล่า | ขัด | ค่อย | ภา | โอ | ่ํา | มัน | ชม | ห์ | ชาย | ัล | นาย | ▁เจ | เสียง | ยิ่ง | รู | ๋ย | เปล | เอา | ▁เส | คง | ตรา | ห้า | ินสมุทร | คอย | หญิง | หนี | ้าน | ญา | คุ | บรร | ▁ประ | กาย | ทหาร | ▁อัน | สิ้น | ทธ | ทอง | ักษ | ลังกา | นิ | พู | ศ์ | ่ว | จา | ใหญ | ที่ยว | มน | ไล | จริง | ▁เจ้า | จํา | ▁บ้าง | บอก | ▁ต่าง | ติ | ▁เข้า | ไม | ศร | อั | เคย | เลี้ยง | กรา | แสน | ▁จน | จับ | พบ | ครั้น | จง | พวก | สี | ไข | ษฐ | เกล | คา | รม | พัก | พัน | ซึ่ง | หนัก | นี | ่าว | กรุง | กล้ง | ▁เหมือน | ครา | เคร | ท้าว | ใส | ▁พวก | ตั้ง | หลง | ล้วน | ▁ไป | ผี | ลํา | นัก | ร้อง | ▁จง | ทรา | หนา | ▁ก็ | กลัว | ▁ที่ | เคียง | อาย | เรือ | ▁แม้น | เต | แค | ยก | พราะ | ใหญ่ | ▁ครั้น | ▁น | แก้ว | ถือ | ▁ได้ | เหลือ'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_vocabs = [bpe.id_to_piece(id) for id in range(bpe.get_piece_size())]\n",
    "\" | \".join(bpe_vocabs[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu6QnnRfQyFj"
   },
   "source": [
    "### User-defined symbols\n",
    "\n",
    "Another important concept to know of is User-defined symbols. These special symbols are reserved for a special purpose \\(e.g.\\, the \\<MASK\\> token used in BERT) and will always be tokenized into one token.\n",
    "\n",
    "Refer to the documentation for ways to add these special tokens to your tokenizer.\n",
    "\n",
    "https://github.com/google/sentencepiece/blob/master/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEFOj62ZEdzT"
   },
   "source": [
    "## Train another tokenizer on another domain\n",
    "\n",
    "Now try training another unigram tokenizer on `pantip_text` and we will use it to compare with the unigram tokenizer we trained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:16:29.024079Z",
     "iopub.status.busy": "2025-01-18T17:16:29.023757Z",
     "iopub.status.idle": "2025-01-18T17:16:36.632789Z",
     "shell.execute_reply": "2025-01-18T17:16:36.632077Z",
     "shell.execute_reply.started": "2025-01-18T17:16:29.024050Z"
    },
    "id": "O7-QkA1eMZFf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Train\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"pantip-train.txt\",\n",
    "    model_prefix=\"unigram-pantip\",\n",
    "    vocab_size=1000,\n",
    "    model_type=\"unigram\",\n",
    ")\n",
    "\n",
    "sp_pantip_unigram = spm.SentencePieceProcessor(model_file=\"unigram-pantip.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5WOVMbONnYv"
   },
   "source": [
    "## Analyse top tokens on different datasets\n",
    "\n",
    "Use your tokenizers to tokenize the datasets and analyse your most common vocabularies (try 300-400 vocabs with len>1). Hint: tokenize your data and count the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:20:40.934523Z",
     "iopub.status.busy": "2025-01-18T17:20:40.934199Z",
     "iopub.status.idle": "2025-01-18T17:20:41.141003Z",
     "shell.execute_reply": "2025-01-18T17:20:41.140307Z",
     "shell.execute_reply.started": "2025-01-18T17:20:40.934501Z"
    },
    "id": "wbfkGcsUrPYS",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top tokens in PAM dataset:\n",
      "มา: 666\n",
      "ไป: 657\n",
      "ให้: 650\n",
      "จะ: 622\n",
      "ไม่: 536\n",
      "ว่า: 520\n",
      "พระ: 462\n",
      "▁จะ: 421\n",
      "ที่: 417\n",
      "เป็น: 387\n",
      "รา: 326\n",
      "ยา: 321\n",
      "อยู่: 314\n",
      "ใจ: 313\n",
      "เข้า: 297\n",
      "\n",
      "Top tokens in Pantip dataset:\n",
      "ที่: 1137\n",
      "ได้: 666\n",
      "ไป: 659\n",
      "มา: 656\n",
      "จะ: 633\n",
      "ก็: 628\n",
      "เรา: 602\n",
      "ว่า: 568\n",
      "มี: 550\n",
      "ไม่: 536\n",
      "เป็น: 522\n",
      "การ: 440\n",
      "ให้: 405\n",
      "ของ: 397\n",
      "้า: 396\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Load SentencePiece models\n",
    "sp_pam_unigram = spm.SentencePieceProcessor(model_file='unigram-pam.model')\n",
    "sp_pantip_unigram = spm.SentencePieceProcessor(model_file='unigram-pantip.model')\n",
    "\n",
    "# Tokenize and count tokens in a dataset\n",
    "def analyze_tokens(dataset, tokenizer, top_n=400, min_len=2):\n",
    "    token_counts = collections.Counter()\n",
    "    \n",
    "    # Tokenize each line in the dataset\n",
    "    for line in dataset:\n",
    "        tokens = tokenizer.encode(line, out_type=str)  # Tokenize into string tokens\n",
    "        token_counts.update(tokens)\n",
    "    \n",
    "    # Filter and sort tokens\n",
    "    filtered_tokens = {token: count for token, count in token_counts.items() if len(token) >= min_len}\n",
    "    sorted_tokens = sorted(filtered_tokens.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top N tokens\n",
    "    return sorted_tokens[:top_n]\n",
    "\n",
    "# Load datasets (replace with actual loading logic)\n",
    "with open('pam-test.txt', 'r', encoding='utf-8') as f:\n",
    "    pam_dataset = f.readlines()\n",
    "\n",
    "with open('pantip-test.txt', 'r', encoding='utf-8') as f:\n",
    "    pantip_dataset = f.readlines()\n",
    "\n",
    "# Analyze tokens\n",
    "pam_top_tokens = analyze_tokens(pam_dataset, sp_pam_unigram)\n",
    "pantip_top_tokens = analyze_tokens(pantip_dataset, sp_pantip_unigram)\n",
    "\n",
    "# Display results\n",
    "print(\"Top tokens in PAM dataset:\")\n",
    "for token, count in pam_top_tokens[:15]:\n",
    "    print(f\"{token}: {count}\")\n",
    "\n",
    "print(\"\\nTop tokens in Pantip dataset:\")\n",
    "for token, count in pantip_top_tokens[:15]:\n",
    "    print(f\"{token}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz0GdZ-5YYM9"
   },
   "source": [
    "### To answer\n",
    "What are some notable differences you see between the two vocabs?\n",
    "\n",
    "Write your answer below.\n",
    "**Ans**: Vocab ใน pantip มีความเป็นภาษาพูดในบทสนทนาในเน็ต ส่วนพระอภัยมณีจะเป็นทางการ และมีคำทางศาสนา ความเชื่อ? มากกว่า (เช่น 'พระ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipjO87HPYl4N"
   },
   "source": [
    "## Using tokenizer across domains\n",
    "\n",
    "One problem you may face is your dataset is very specialized. In that case the tokenizer trained on a general domain may not perform as good as it should when used on your dataset.\n",
    "\n",
    "Next you will try using tokenizers trained on one general domain (on Pantip) and use it on a specialized domain (พระอภัยมณี) and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4_6JG_l5BXh"
   },
   "source": [
    "### Q3 MCV\n",
    "\n",
    "What percentage increase do you observe when tokenizing the whole พระอภัยมณี dataset with a tokenizer trained on Pantip compared to the one trained on พระอภัยมณี."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:21:11.978364Z",
     "iopub.status.busy": "2025-01-18T17:21:11.978004Z",
     "iopub.status.idle": "2025-01-18T17:21:27.419635Z",
     "shell.execute_reply": "2025-01-18T17:21:27.418637Z",
     "shell.execute_reply.started": "2025-01-18T17:21:11.978325Z"
    },
    "id": "3tCh1RaZrTAM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_writer_ = io.BytesIO()\n",
    "spm.SentencePieceTrainer.train(sentence_iterator=iter(pam_train_text), model_writer=model_writer_, vocab_size = 1000)\n",
    "sp_pam = spm.SentencePieceProcessor(model_proto=model_writer_.getvalue())\n",
    "\n",
    "model_writer_ = io.BytesIO()\n",
    "spm.SentencePieceTrainer.train(sentence_iterator=iter(pantip_train_text), model_writer=model_writer_, vocab_size = 1000)\n",
    "sp_pantip = spm.SentencePieceProcessor(model_proto=model_writer_.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:21:38.285846Z",
     "iopub.status.busy": "2025-01-18T17:21:38.285562Z",
     "iopub.status.idle": "2025-01-18T17:21:39.240336Z",
     "shell.execute_reply": "2025-01-18T17:21:39.239305Z",
     "shell.execute_reply.started": "2025-01-18T17:21:38.285825Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4342710745209495\n"
     ]
    }
   ],
   "source": [
    "pam_data_all = ' '.join(pra_apai_manee_data)\n",
    "print(\n",
    "    len(sp_pantip.encode(pam_data_all)) / len(sp_pam.encode(pam_data_all)) - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duaCJRO96SX1"
   },
   "source": [
    "### Q4 MCV\n",
    "\n",
    "What percentage increase do you observe when tokenizing the whole Pantip dataset with a tokenizer trained on พระอภัยมณี compared to the one trained on Pantip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:21:42.990407Z",
     "iopub.status.busy": "2025-01-18T17:21:42.990102Z",
     "iopub.status.idle": "2025-01-18T17:21:43.296057Z",
     "shell.execute_reply": "2025-01-18T17:21:43.295197Z",
     "shell.execute_reply.started": "2025-01-18T17:21:42.990385Z"
    },
    "id": "axk9gOIgrTYd",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12855248847916112\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sum(map(len, sp_pam.encode(pantip_text))) / sum(map(len, (sp_pantip.encode(pantip_text)))) - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZYKuamv7-wI"
   },
   "source": [
    "### To answer\n",
    "Why do you think the number of tokens tokenized by the general tokenizer (the one trained on Pantip) has a higher percentage increase compared to the number of tokens tokenized by the specialized tokenizer? (Hint: we fixed vocab size.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans**: เพราะ fix vocab size, general tokenizer ไม่ได้เคยเจอคำยาว ๆ ใน PAM เลยหั่นคำถี่กว่า ส่วน specialized tokenizer ไม่ค่อยเจอคำ general ใน pantip เลยหั่นคำน้อยกว่า"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7j_Cc0p9-5S"
   },
   "source": [
    "## The effect on language models\n",
    "\n",
    "Next, we will see the effect of using \"cross-domain\" tokenizers on Language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiWztANvohhn"
   },
   "source": [
    "### Setup\n",
    "We are going to reuse the code from the last assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:21:48.906693Z",
     "iopub.status.busy": "2025-01-18T17:21:48.906415Z",
     "iopub.status.idle": "2025-01-18T17:21:53.882185Z",
     "shell.execute_reply": "2025-01-18T17:21:53.881162Z",
     "shell.execute_reply.started": "2025-01-18T17:21:48.906673Z"
    },
    "id": "7pVtSbmVpwOo",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightning\n",
      "  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.9.0)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.9)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.1)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.0.post0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.10)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.16.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\n",
      "Downloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightning\n",
      "Successfully installed lightning-2.5.0.post0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:21:53.883907Z",
     "iopub.status.busy": "2025-01-18T17:21:53.883576Z",
     "iopub.status.idle": "2025-01-18T17:22:02.701656Z",
     "shell.execute_reply": "2025-01-18T17:22:02.700970Z",
     "shell.execute_reply.started": "2025-01-18T17:21:53.883855Z"
    },
    "id": "JMt5GzLrW4x3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import lightning as L\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:23:30.220568Z",
     "iopub.status.busy": "2025-01-18T17:23:30.220226Z",
     "iopub.status.idle": "2025-01-18T17:23:30.226213Z",
     "shell.execute_reply": "2025-01-18T17:23:30.225232Z",
     "shell.execute_reply.started": "2025-01-18T17:23:30.220543Z"
    },
    "id": "0OIs_VS_oo1M",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "  def __init__(self, data, tokenizer, seq_len = 128):\n",
    "\n",
    "    token_ids = [tokenizer.encode(d, add_bos=True, add_eos=True) for d in data]\n",
    "    flatten_token_ids = list(itertools.chain(*token_ids))\n",
    "    encoded = torch.LongTensor(flatten_token_ids)\n",
    "\n",
    "    left_over = len(encoded) % seq_len\n",
    "    encoded = encoded[:len(encoded)-left_over]\n",
    "    self.encoded = encoded.view(-1, seq_len)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.encoded[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:23:57.242567Z",
     "iopub.status.busy": "2025-01-18T17:23:57.242206Z",
     "iopub.status.idle": "2025-01-18T17:23:57.251148Z",
     "shell.execute_reply": "2025-01-18T17:23:57.250132Z",
     "shell.execute_reply.started": "2025-01-18T17:23:57.242539Z"
    },
    "id": "hk6vEPiMq34n",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LSTM(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, learning_rate, criterion):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size=vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        src = batch[:, :-1]\n",
    "        target = batch[:, 1:]\n",
    "        prediction = self(src)\n",
    "        prediction = prediction.reshape(-1, self.vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "\n",
    "        src = batch[:, :-1]\n",
    "        target = batch[:, 1:]\n",
    "        with torch.no_grad():\n",
    "          prediction = self(src)\n",
    "        prediction = prediction.reshape(-1, self.vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:24:01.885192Z",
     "iopub.status.busy": "2025-01-18T17:24:01.884737Z",
     "iopub.status.idle": "2025-01-18T17:24:01.891371Z",
     "shell.execute_reply": "2025-01-18T17:24:01.890366Z",
     "shell.execute_reply.started": "2025-01-18T17:24:01.885156Z"
    },
    "id": "oKhuOygixndB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = sp_pam.get_piece_size()\n",
    "embedding_dim = 200\n",
    "hidden_dim = 512\n",
    "num_layers = 3\n",
    "dropout_rate = 0.2\n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_batch_size = 64\n",
    "test_batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOtOE7mr-heY"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8-x9HiPDcpE"
   },
   "source": [
    "<a name=\"no1\"></a>\n",
    "#### 1. Training on Pantip data with Pantip tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:24:04.132670Z",
     "iopub.status.busy": "2025-01-18T17:24:04.132384Z",
     "iopub.status.idle": "2025-01-18T17:25:02.886155Z",
     "shell.execute_reply": "2025-01-18T17:25:02.885318Z",
     "shell.execute_reply.started": "2025-01-18T17:24:04.132649Z"
    },
    "id": "oUv_A4MTx0Ob",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 200 K  | train\n",
      "1 | lstm      | LSTM             | 5.7 M  | train\n",
      "2 | dropout   | Dropout          | 0      | train\n",
      "3 | fc        | Linear           | 513 K  | train\n",
      "4 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 M     Total params\n",
      "25.511    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38de2ed965e4a26987a823adef799b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pantip)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pantip)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pantip)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pantip)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pantip_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:25:07.526819Z",
     "iopub.status.busy": "2025-01-18T17:25:07.526484Z",
     "iopub.status.idle": "2025-01-18T17:25:13.968746Z",
     "shell.execute_reply": "2025-01-18T17:25:13.968092Z",
     "shell.execute_reply.started": "2025-01-18T17:25:07.526790Z"
    },
    "id": "1e-Y1_GYy65g",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad6d70d663c4bf8a7c8dc8114f35c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on Pantip train set is:\t75.2471756401668\n",
      "Perplexity on Pra apai manee train set is:\t108.86476152085957\n",
      "Perplexity on Pantip test set is:\t107.75407335008728\n",
      "Perplexity on Pra apai manee test set is:\t110.29855257219891\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s3AmE4nDjmL"
   },
   "source": [
    "<a name=\"no2\"></a>\n",
    "#### 2. Training on Pantip data with Pra apai manee tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:25:37.561539Z",
     "iopub.status.busy": "2025-01-18T17:25:37.561188Z",
     "iopub.status.idle": "2025-01-18T17:26:57.199619Z",
     "shell.execute_reply": "2025-01-18T17:26:57.199008Z",
     "shell.execute_reply.started": "2025-01-18T17:25:37.561510Z"
    },
    "id": "vfRdW3m1Dmj_",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 200 K  | train\n",
      "1 | lstm      | LSTM             | 5.7 M  | train\n",
      "2 | dropout   | Dropout          | 0      | train\n",
      "3 | fc        | Linear           | 513 K  | train\n",
      "4 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 M     Total params\n",
      "25.511    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd25edc4259249a3bf1961317db67d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pam)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pam)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pam)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pam)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pantip_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:27:00.248193Z",
     "iopub.status.busy": "2025-01-18T17:27:00.247861Z",
     "iopub.status.idle": "2025-01-18T17:27:06.283127Z",
     "shell.execute_reply": "2025-01-18T17:27:06.282238Z",
     "shell.execute_reply.started": "2025-01-18T17:27:00.248168Z"
    },
    "id": "xwLN1IarD3g9",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d51e93dc6a4b249558d090bcd25a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on Pantip train set is:\t39.76091739063008\n",
      "Perplexity on Pra apai manee train set is:\t467.39616606725286\n",
      "Perplexity on Pantip test set is:\t49.34152609503775\n",
      "Perplexity on Pra apai manee test set is:\t439.5797884722374\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB8zqptTWcA6"
   },
   "source": [
    "#### To answer\n",
    "\n",
    "The perplexity numbers should indicate that:\n",
    "1. Training the LM with Pra apai manee tokenizer on Pantip (no. [2](#no2)) results in overfitting to Pantip and poor generalization to the Pra apai manee dataset.\n",
    "2. However using the Pantip tokenizer (no. [1](#no1)) results in a much better generalization.\n",
    "\n",
    "Try and come up with some reasons for the results above. <br>\n",
    "Hint:\n",
    "1. think about \"general\" vocabs and domain-specific vocabs.\n",
    "2. what do you think happens to the model when the token ids become longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans**: Pantip tokenizer generalize กว่า เพราะคำที่พูดใน pantip เป็นบทสนทนาที่ versatile, แล้วก็ภาษาพูดมักจะเป็นคำสั้น ๆ ทำให้หั่นคำได้ดี ส่วน PAM tokenizer overfit ใน domain ตัวเอง เลย handle คำที่ว ๆ ไปได้ไม่ค่อยดี"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8VPMm7pLdSl"
   },
   "source": [
    "\n",
    "<a name=\"no3\"></a>\n",
    "#### 3. Training on Pra apai manee data with Pantip tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:27:11.801224Z",
     "iopub.status.busy": "2025-01-18T17:27:11.800786Z",
     "iopub.status.idle": "2025-01-18T17:28:55.139615Z",
     "shell.execute_reply": "2025-01-18T17:28:55.138933Z",
     "shell.execute_reply.started": "2025-01-18T17:27:11.801184Z"
    },
    "id": "oR5fp-YCLnnU",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 200 K  | train\n",
      "1 | lstm      | LSTM             | 5.7 M  | train\n",
      "2 | dropout   | Dropout          | 0      | train\n",
      "3 | fc        | Linear           | 513 K  | train\n",
      "4 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 M     Total params\n",
      "25.511    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadf4678c1444e9e9908c83b9e18cf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pantip)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pantip)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pantip)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pantip)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pam_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:28:55.141295Z",
     "iopub.status.busy": "2025-01-18T17:28:55.140972Z",
     "iopub.status.idle": "2025-01-18T17:29:02.444303Z",
     "shell.execute_reply": "2025-01-18T17:29:02.443617Z",
     "shell.execute_reply.started": "2025-01-18T17:28:55.141262Z"
    },
    "id": "f_LhF7w7Lxwo",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133a69034f14406fa6410554ad5dd508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on Pantip train set is:\t3858.9745881465105\n",
      "Perplexity on Pra apai manee train set is:\t33.82976666316134\n",
      "Perplexity on Pantip test set is:\t3513.2443175066346\n",
      "Perplexity on Pra apai manee test set is:\t36.576187754993896\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apk9crJjMLoW"
   },
   "source": [
    "<a name=\"no4\"></a>\n",
    "#### 4. Training on Pra apai manee data with Pra apai manee tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:29:02.445683Z",
     "iopub.status.busy": "2025-01-18T17:29:02.445385Z",
     "iopub.status.idle": "2025-01-18T17:30:18.502759Z",
     "shell.execute_reply": "2025-01-18T17:30:18.502108Z",
     "shell.execute_reply.started": "2025-01-18T17:29:02.445658Z"
    },
    "id": "_G7GMBIKLzGK",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 200 K  | train\n",
      "1 | lstm      | LSTM             | 5.7 M  | train\n",
      "2 | dropout   | Dropout          | 0      | train\n",
      "3 | fc        | Linear           | 513 K  | train\n",
      "4 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 M     Total params\n",
      "25.511    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9274eb6ae51b47b4b763baf50df7997f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pam)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pam)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pam)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pam)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pam_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:30:18.504001Z",
     "iopub.status.busy": "2025-01-18T17:30:18.503736Z",
     "iopub.status.idle": "2025-01-18T17:30:24.831110Z",
     "shell.execute_reply": "2025-01-18T17:30:24.830417Z",
     "shell.execute_reply.started": "2025-01-18T17:30:18.503967Z"
    },
    "id": "9H753o_JMRFw",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f7713aa27748919558137426e27885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on Pantip train set is:\t489.38666574549967\n",
      "Perplexity on Pra apai manee train set is:\t71.82827235291751\n",
      "Perplexity on Pantip test set is:\t480.680747612356\n",
      "Perplexity on Pra apai manee test set is:\t80.13752648494753\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en9Lmmj4dZ-1"
   },
   "source": [
    "#### To answer\n",
    "\n",
    "The perplexity numbers should indicate that:\n",
    "1. Both LM overfits on Pra apai manee data and performs really bad on Pantip data.\n",
    "2. However using the Pra apai manee tokenizer (no. [4](#no4)) results in a  better generalization than the Pantip tokenizer(no. [3](#no3)).\n",
    "\n",
    "Try and come up with some reasons for the results above. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans**: เพราะเอามาใช้ใน domain-specific task, ตอนนี้ PAM tokenizer เลยดีกว่า เพราะ capture pattern คำใน domain นี้ได้ดีกว่า\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
